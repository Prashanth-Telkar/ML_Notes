# SVM : SUPPORT VECTOR CLASSIFIER

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold,cross_val_score,GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

pd.read_csv('https://cdn.upgrad.com/UpGrad/temp/3389523a-5e5a-479d-9f6a-27a0a63eb17e/Spam.txt',sep=',',header=None)

df_train,df_test=train_test_split(df,train_size=0.7,random_state=100)
df_train

y_train=df_train.pop('spam')
X_train=df_train
y_test=df_test.pop('spam')
X_test=df_test

sc=MinMaxScaler()

var=list(X_train.columns)

X_train[var]=sc.fit_transform(X_train[var])

X_test[var]=sc.transform(X_test[var])

# model initiation and fitting

model=SVC(C=1)	#here C is the cost
model.fit(X_train,y_train)

y_train_pred=model.predict(X_train)
y_test_pred=model.predict(X_test)

accuracy_score(y_test,y_test_pred)
precision_score(y_test,y_test_pred)
confusion_matrix(y_test,y_test_pred)

Interpretation of Results
In the confusion matrix, the elements at (0, 0) and (1,1) correspond to the more frequently occurring class, i.e. ham emails. Thus, it implies that:

92% of all emails are classified correctly
88.5% of spams are identified correctly (sensitivity/recall)
Specificity, or % of hams classified correctly, is 95%
***********************************************************************************************************
# Hyperparameter Tuning

K-Fold Cross Validation:
Let's first run a simple k-fold cross validation to get a sense of the average metrics as computed over multiple folds. the easiest way to do cross-validation is to use the 
cross_val_score() function.

# creating a KFold object with 5 splits 
folds = KFold(n_splits = 5, shuffle = True, random_state = 4)

# instantiating a model with cost=1
model = SVC(C = 1)

cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = 'accuracy') 

# print 5 accuracies obtained from the 5 folds
print(cv_results)
print("mean accuracy = {}".format(cv_results.mean()))
**********************************************************************************************************
# Grid Search to Find Optimal Hyperparameter C:

K-fold CV helps us compute average metrics over multiple folds, and that is the best indication of the 'test accuracy/other metric scores' we can have.
But we want to use CV to compute the optimal values of hyperparameters (in this case, the cost C is a hyperparameter). This is done using the GridSearchCV() method, 
which computes metrics (such as accuracy, recall etc.)
In this case, we have only one hyperparameter, though you can have multiple, such as C and gamma in non-linear SVMs. In that case, you need to search through a grid of 
multiple values of C and gamma to find the optimal combination, and hence the name GridSearchCV.

# specify range of parameters (C) as a list
params = {"C": [0.1, 1, 10, 100, 1000]}

model = SVC()

# set up grid search scheme
# note that we are still using the 5 fold CV scheme we set up earlier

model_cv = GridSearchCV(estimator = model, param_grid = params, 
                        scoring= 'accuracy', 
                        cv = folds, 
                        verbose = 1,
                       return_train_score=True) 

# fit the model - it will fit 5 folds across all values of C
model_cv.fit(X_train, y_train)  

# results of grid search CV
cv_results = pd.DataFrame(model_cv.cv_results_)
cv_results

# plot of C versus train and test scores

plt.figure(figsize=(8, 6))
plt.plot(cv_results['param_C'], cv_results['mean_test_score'])
plt.plot(cv_results['param_C'], cv_results['mean_train_score'])
plt.xlabel('C')
plt.ylabel('Accuracy')
plt.legend(['test accuracy', 'train accuracy'], loc='upper left')
plt.xscale('log')

Though the training accuracy monotonically increases with C, the test 
accuracy gradually reduces. Thus, we can conclude that higher values of 
C tend to overfit the model. This is because a high C value aims to classify all training examples correctly (since C is the cost of misclassification - 
if you impose a high cost on the model, it will avoid misclassifying any points by overfitting the data).

best_score = model_cv.best_score_
best_C = model_cv.best_params_['C']

*************************************************************************************************************
# Optimising for Other Evaluation Metrics:
In this case, we had optimised (tuned) the model based on overall accuracy, though that may not always be the best metric to optimise. For example, if you are concerned more about 
catching all spams (positives), 
you may want to maximise TPR or sensitivity/recall. If, on the other hand, you want to avoid classifying hams as spams (so that any important mails don't get into the spam box), 
you would maximise the TNR or specificity.

# specify params
params = {"C": [0.1, 1, 10, 100, 1000]}

# specify scores/metrics in an iterable
scores = ['accuracy', 'precision', 'recall']

for score in scores:
    print("# Tuning hyper-parameters for {}".format(score))
    
    # set up GridSearch for score metric
    clf = GridSearchCV(SVC(), 
                       params, 
                       cv=folds,
                       scoring=score,
                       return_train_score=True)
    # fit
    clf.fit(X_train, y_train)

    print(" The highest {0} score is {1} at C = {2}".format(score, clf.best_score_, clf.best_params_))
    print("\n")

Thus, you can see that the optimal value of the hyperparameter varies significantly with the choice of evaluation metric.