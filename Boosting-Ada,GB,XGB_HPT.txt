# BOOSTING and HYPERPARAMETER TUNING (HPT)

# ADABOOST: ADAPTIVE BOOSTING 

import sklearn
from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

import xgboost
from xgboost import XGBRegressor

xgb=XGBRegressor()
params = {
        'n_estimators' : [100, 200, 500, 750], # no of trees 
        'learning_rate' : [0.01, 0.02, 0.05, 0.1, 0.25],  # eta
        'min_child_weight': [1, 5, 7, 10],
        'gamma': [0.1, 0.5, 1, 1.5, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5, 10, 12]
        }

folds = 3

param_comb = 1000 	#It will randomly pick any 1000 combinations of the above hyperparameters

random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='r2', n_jobs=-1, cv=3, verbose=1, random_state=42)
random_search.fit(X_train,y_train)

xgb_best=random_search.best_estimator_

y_pred_train_xgb_best=xgb_best.predict(X_train)
y_pred_test_xgb_best=xgb_best.predict(X_test)
print(r2_score(y_train,y_pred_train_xgb_best))
print(r2_score(y_test,y_pred_test_xgb_best))

f=pd.DataFrame({'features':X_train.columns,'importance':xgb_best.feature_importances_}).sort_values(by='importance',ascending=False)
                        
