# K Means Clustering:

# Extra Importing

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage,dendrogram,cut_tree
import datetime as dt

*********************************************************************************************************
Create KPI(key performance indicators) dataframe : Ex: Recency, Frequency, Monetary:

df.InvoiceDate=pd.to_datetime(df.InvoiceDate,format='%d-%m-%Y %H:%M')
max_date=max(df.InvoiceDate)
df['diff']=max_date - df['InvoiceDate']
resency=df.groupby('CustomerID')['diff'].min()
resency=resency.reset_index()
resency['diff']=resency['diff'].dt.days
**********************************************************************************************************

# Outllier treatment

Q1=mf.amount.quantile(0.05)
Q2=mf.amount.quantile(0.95)
IQR=Q2-Q1
mf=mf[(mf.amount>=(Q1-(1.5*IQR))) & (mf.amount<=(Q2+(1.5*IQR)))]

Q1=mf.freq.quantile(0.05)
Q2=mf.freq.quantile(0.95)
IQR=Q2-Q1
mf=mf[(mf.freq>=(Q1-(1.5*IQR))) & (mf.freq<=(Q2+(1.5*IQR)))]

Q1=mf.recency.quantile(0.05)
Q2=mf.recency.quantile(0.95)
IQR=Q2-Q1
mf=mf[(mf.recency>=(Q1-(1.5*IQR))) & (mf.recency<=(Q2+(1.5*IQR)))]
final_mf=mf.copy()
mf

********************************************************************************************************
scaler=StandardScaler()
var=['amount','freq','recency']
mf[var]=scaler.fit_transform(mf[var])
mf

********************************************************************************************************************************************
Checking the clustering tendency using Hopkins Statistics:

Hopkins Statistics:
The Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.

If the value is between {0.01, ...,0.3}, the data is regularly spaced.

If the value is around 0.5, it is random.

If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan

def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) 
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
    rand_X = sample(range(0, n, 1), m)
 
    ujd = []
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
 
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H

hopkins(mf)
output: 0.95625

*********************************************************************************************************************************************************
# Model Building
# Kmeans Clustering

km=KMeans(n_clusters=x,max_iter=50).fit(mf)

# elbow-curve/SSD (sum of squared distance)

k=[2,3,4,5,6,7,8]
ss=[]
for x in k:
    km=KMeans(n_clusters=x,max_iter=50).fit(mf)
    ss.append(km.inertia_)
sns.lineplot(k,ss)
plt.show()

# Silhouette Analysis:

k=[2,3,4,5,6,7,8]
for x in k:
    km=KMeans(n_clusters=x,max_iter=50).fit(mf)
    print(f'for k={x} the silhouette_score is {silhouette_score(mf,km.labels_)}')

# To check the cluster lables:

mf['cluster_label']=km.labels_
*******************************************************************************************

# 3D plot:

zero=mf[mf.cluster_label==0]
one=mf[mf.cluster_label==1]
two=mf[mf.cluster_label==2]

from mpl_toolkits.mplot3d import Axes3D
fig=plt.figure(figsize=[15,15])
ax=fig.add_subplot(111,projection='3d')

x=list(zero.amount.values)
y=list(zero.freq.values)
z=list(zero.recency.values)
ax.scatter(x,y,z,c='r',marker='o')

********************************************************************************************

# Bar plot for final conclusions:

plt.figure(figsize=[15,10])
plt.subplot(2,2,1)
sns.countplot(data=final_mf,x='cluster_label')

plt.subplot(2,2,2)
sns.boxplot(data=final_mf,x='cluster_label',y='amount')

plt.subplot(2,2,3)
sns.boxplot(data=final_mf,x='cluster_label',y='freq')

plt.subplot(2,2,4)
sns.boxplot(data=final_mf,x='cluster_label',y='recency')
plt.show()

*************************************************************************************************************

# Hierarchical Clustering:

merging=linkage(mf,method='single')   #[method can be: 'single','complete','average']
dendrogram(merging)
plt.show()

# Cuttiing the tree to form n-clusters:

cluster_lables_h=cut_tree(merging,n_clusters=3).reshape(-1,)
mf['cluster_lables_h']=cluster_lables_h
mf

*****************************************************************************************************

Combination of KMeans and Hierarchical Clustering

## INDUSTRY PRACTICE:

- IN INDUSTRU FIRST THEY START WITH HIERARCHICAL CLUSTERING.
- DECIDE THE NUMBER OF CLUSTERS LOOKING AT THE DENDOGRAM (ALSO THE RESULTS OF K=3 TO K=15 CAN BE 
  STORED IN AN EXCEL SHEET AND CAN BE ANALYSED TO FIND THE BEST K.)
- AFTER DECIDING THE BEST K, FIND THE AVERAGE OF ALL THE KPIs FOR CORRESPONDING CLUSTER LABLES.
- THESE AVERAGE WILL BE FED TO KMeans ALGORITHM AS INITIAL CENTROIDS.
- FINALLY KMeans MODEL IS BUILT USING THESE INITIAL CENTROIDS.

THE INITIAL CENTROID IS CALCULATED AS BELOW:

# Average of each feature in each cluster: Assigned as initial centroid for KMeans Algorithm.

initial_centroids=pd.pivot_table(data=mf,values=['amount','freq','recency'],index='cluster_lables_h')
initial_centroids
km=KMeans(n_clusters=3,init=initial_centroids,max_iter=50).fit(mf)


