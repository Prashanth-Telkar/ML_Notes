# IMPORTANT SHORT NOTES:

# Inferenctial Statistics: 
- Inferential statistics: Making inferences about the population using the sample data
- Inferential Sstatistics is a process of “inferring” insights from sample data is called “Inferential Statistics”.
- The expected value should be interpreted as the average value you get after the experiment has been conducted an infinite number of times. 
  For example, the expected value for the number of red balls is 2.385. This means that if we conduct the experiment (play the game) infinite times, 
  the average number of red balls per game would end up being 2.385.
- In the long run (i.e. if it is played a lot of times), is this game profitable for the players or for the house? Or will everybody break even in the long run?
  Recall that we established a three-step process for answering this question:
	Find all the possible combinations
	Find the probability of each combination
	Use the probabilities to estimate the profit/loss per player
- Random variable: 
	          random variable X basically converts outcomes of experiments to something measurable.
- Expected value:The expected value should be interpreted as the average value you get after the experiment has been conducted an infinite number of times. 
- Probability distribution: probability distribution is a distribution giving us the probability for all possible values of X
- Mathematically speaking, for a random variable X that can take values x1,x2,x3,...........,xn, the expected value (EV) is given by:
  EV(X)=x1∗P(X=x1)+x2.P(X=x2)+x3∗P(X=x3)+...........+xn∗P(X=xn)
- probability distribution is a distribution giving us the probability for all possible values of X.
- So basically, a probability distribution is ANY form of representation that tells us the probability for all possible values of X.
- Hence, in a valid, complete probability distribution, there are no negative values, and the total of all probability values adds up to 1
- probability distribution and frequency distribution would be exactly similar in shape, just with different scales.
- In other words, if we conduct the experiment (play the game) infinite times, the average money won by a player would be ₹11.28. Hence, we decided that we should either decrease 
  the prize money or increase the penalty to make the expected value of X negative. A negative expected value would imply that, on average, a player would be expected to lose money 
  and the house would profit.
- A roulette wheel is a game found in many casinos : Europian roulette wheel numbers(0-36)(choose btw 0-36)  , American roulette wheel numbers(0-36,00)(choose btw 1-36).  
- Discrete Probability distribution: 1. Binomial PD, 2. Uniform PD 
- Probability distributions that are commonly used for discrete random variables, such as the binomial probability distribution and the uniform probability distribution. 
  Also, you will learn the concept of cumulative probability which will be very useful in our next session on continuous probability distributions.
- finding the probability without conducting any experiment. You saw that these theoretical (calculated) values of probability are actually quite close to the experimental 
  values that we got. The small differences that you can notice exist because of the low number of experiments done.
- the formula for finding binomial probability is given by - P(X=r)= nCr[(p)^r][(1−p)^(n−r)]
 	Where n is no. of trials, p is probability of success and r is no. of successes after n trials.
- Conditions that need to be followed in order for us to be able to apply the formula.
	Total number of trials is fixed at n
	Each trial is binary, i.e., has only two possible outcomes - success or failure
	Probability of success is same in all trials, denoted by p

- 	Binomial Distribution Applicable						Binomial Distribution Not Applicable
	-Tossing a coin 20 times to see how many tails occur				-Tossing a coin until a heads occurs
	-Asking 200 randomly selected people if they are older than 21 or not		-Asking 200 randomly selected people how old they are
	-Drawing 4 red balls from a bag, putting each ball back after drawing it	-Drawing 4 red balls from a bag, not putting each ball back after drawing it
- binomial distribution is applicable in situations where there are a fixed number of yes or no questions, with the probability of a yes or a no remaining the same in all the questions
- we only discussed the probability of getting an exact value. For example, we know the probability of X = 4 (4 red balls). But what if the house wants to know the probability of getting 
  < 3 red balls, as the house knows that for < 3 red balls, the players will lose and they will make money?
- Sometimes, talking in less than is more useful, for example — how many employees can get to work in less than 40 minutes?
- The cumulative probability of X, denoted by F(x), is defined as the probability of the variable being less than or equal to x.
- Cumulative probability Destribution(CPD) can be drawn for both continuous and descrite PD. In case of descrite PD the CPD curve will be step curve from 0 to 1. In case of continuous 
  PD the CPD curve will be smooth curve from 0 to 1.
- binomial distribution: It can be used to find the probability of any kind of event, if that event is a series of yes or no questions, with the probability of yes being the same 
  for all questions. 
- Cumulative probability of x, denoted by F(x), which is the probability that the random variable X takes a value less than or equal to x.
- Continuous Probability Distributions:
- CDF(cumulative distribution function) and PDF(probability density function), these two functions talk about probabilities in terms of intervals rather than exact values, it is 
  advisable to use them when talking about continuous random variables,and not the bar chart distribution that we used for discrete variables.
- Just to recall, a CDF, or a cumulative distribution function, is a distribution which plots the cumulative probability of X against X.
- A PDF, or Probability Density Function, however, is a function in which the area under the curve, gives you the cumulative probability.
- For example, the area under the curve, between 20, the smallest possible value of X and 28, gives the cumulative probability for X = 28.
- The main difference between the cumulative probability distribution of a continuous random variable and a discrete one, is the way you plot them. While the continuous variables’ 
  cumulative distribution is a curve, the distribution for discrete variables looks more like a bar chart:
- The reason for showing both of these so differently is that, for discrete variables, the cumulative probability does not change very frequently. In the discrete example, we only care 
  about what the probability is for 0, 1, 2, 3 and 4. This is because the cumulative probability will not change between, say, 3 and 3.999999. For all values between these two, the 
  cumulative probability is equal to 0.8704.
- However, for the continuous variable, i.e. the daily commute time, you have a different cumulative probability value for every value of X. For example, the value of cumulative 
  probability at 21 will be different from its value at 21.1, which will again be different from the one at 21.2 and so on. Hence, you would show its cumulative probability as a 
  continuous curve, not a bar chart.
- A commonly observed type of distribution among continuous variables is the uniform distribution
- Since all possible values are between 0 and 10, the area under the curve between 0 and 10 is equal to 1.
- Clearly, this area is the area of a rectangle with length 10 and unknown height h. Hence, you can say that 10*h = 1, which gives us h = 0.1. So, the value of the PDF for all values 
  between 0 and 10 is 0.1.
- Now, I’m sure you are wondering, when to use PDFs and when to use CDFs? They are both good for continuous variables, but which one is used more in real life analysis?
- Well, PDFs are more commonly used in real life. The reason is that it is much easier to see patterns in PDFs as compared to CDFs. For example, here are the PDF and the CDF of a 
  uniformly distributed continuous random variable:
- The PDF clearly shows uniformity, as the probability density’s value remains constant for all possible values. However, the CDF does not show any trends that help you identify quickly 
  that the variable is uniformly distributed.
- Now, let’s see the PDF and the CDF of a symmetrically distributed continuous random variable: (inf stat-- CPD--PDF II)
- Again, it is clear that the symmetrical nature of the variable is much more apparent in the PDF than in the CDF. 
- Hence, generally, PDFs are used more commonly than CDFs.
- You can say that P(X ≤ 175.3 cm) = P(X < 175.3 cm) + P(X = 175.3 cm). Now, since X is a continuous variable, you know that the probability of getting an exact value is zero. Hence, 
  P(X=175.3 cm) = 0, which means that P(X ≤ 175.3 cm = P(X < 175.3 cm) + 0.
- NORMAL DISTRIBUTION:
- All data that is normally distributed follows the 1-2-3 rule. This rule states that there is a -
	68% probability of the variable lying within 1 standard deviation of the mean
	95% probability of the variable lying within 2 standard deviations of the mean
	99.7% probability of the variable lying within 3 standard deviations of the mean
- A lot of naturally occurring variables are normally distributed. For example, the heights of a group of adult men would be normally distributed. 
- STANDARD NORMAL DISTRIBUTION: 
- it doesn’t matter what the value of µ and σ is. All you need to know, if you want to find the probability, is how far the value of X is from µ — specifically, what multiple of σ is 
  the difference between X and µ.
- As you just learnt, the standardised random variable is an important parameter. It is given by:Z=(X−μ)/σ   Z-score or  standard normal variable.
- Basically, it tells you how many standard deviations away from the mean your random variable is. As you just saw, you can find the cumulative probability corresponding to a 
  given value of Z, using the Z table:
- Not only that, you can also use Excel or Python to find the cumulative probability for Z. For example, let’s say you want to find the cumulative probability for Z = 1.5. 
  In Excel, you would type: = NORM.S.DIST(1.5, TRUE)
  Basically, the syntax is: = NORM.S.DIST(z, TRUE)
- Here, z is the value of the Z score for which you want to find the cumulative probability. TRUE = find cumulative probability, FALSE = find probability density.
- Also, you can find the probability without standardising. Let’s say that X is normally distributed, with mean (μ) = 35 and standard deviation (σ) = 5. 
  Now, if you want to find the cumulative probability for X = 30, you would type: = NORM.DIST(30, 35, 5, TRUE)
- Basically, the syntax is: = NORM.DIST(x, mean, standard_dev, TRUE)
- As you can see, the value of σ is an indicator of how wide the graph is. This will be true for any graph, not just the normal distribution. A low value of σ means that the 
  graph is narrow, while a high value implies that the graph is wider. This will happen because the wider graph will clearly have more values away from the mean, resulting in 
  a high standard deviation.
- What is the probability of a normally distributed random variable lying within 1.65 standard deviations of the mean?
- You have to find the probability of the variable lying between μ-1.65σ and μ+1.65σ. i.e. P(μ-1.65σ < X < μ+1.65σ). In terms of Z, this becomes P(-1.65 < Z < +1.65). 
  This would be equal to P(1.65) - P(-1.65) = 0.95 - 0.05 = 0.90.
- for a continuous random variable, the probability of getting an exact value is very low, almost zero. Hence, when talking about the probability of continuous random variables, 
  you can only talk in terms of intervals. For example, for a particular company, the probability of an employee’s commute time being exactly equal to 35 minutes was zero, but the 
  probability of an employee having a commute time between 35 and 40 minutes was 0.2.
- Hence, for continuous random variables, probability density functions (PDFs) and cumulative distribution functions (CDFs) are used, instead of the bar chart type of distribution used 
  for the probability of discrete random variables. These functions are preferred because they talk about probability in terms of intervals. 
- you can find the cumulative probability directly by checking the value at x. However, for a PDF, you need to find the area under the curve between the lowest value and x to find the 
  cumulative probability.
- However, you also learnt that PDFs are still more commonly used, mainly because it is very easy to see patterns in them. For example, for a uniformly distributed variable, the PDF 
 and CDF look like this
- While the fact that the variable is uniformly distributed is clear from the PDF, the CDF does not offer any such quick insights.
- The normal distribution. You saw that it is symmetric and its mean, median and mode lie at the centre.
- to find the probability, you do not need to know the value of the mean or the standard deviation — it is enough to know the number of standard deviations away from 
  the mean your random variable is
- The normal distribution finds use in many statistical analyses. The normal distribution, aka the Gaussian distribution.

- CENTRAL LIMIT THEOREM:
- what a sample is and why it is so error-prone. You will then learn how to quantify this error made in sampling, using a very popular theorem in statistics, called the central 
  limit theorem.
- Instead of finding the mean and standard deviation for the entire population, it is sometimes beneficial to find the mean and standard deviation for only a small 
  representative sample. You may have to do this because of time and/or money constraints.
- Sample: 
-			number of entries	Mean						Variance
 Population		N			μ=∑(Xi)/N 					σ^2=∑[(Xi-μ)^2]/N
 Sample			n			X-bar=∑(Xi)/n					S^2=∑[(Xi-(X-bar))^2]/(n-1)
 Sampling distribution	no predefined		μ¯X=∑[X-bar]/(number of samplings) ~= μ		[S.E(Standard Error)]=σ/√n    (σ= population Std Deviation)
- Sanmpling Distribution: the sampling distribution, specifically the sampling distribution of the sampling means, is a probability density function for the sample means of a population.
- The sampling distribution’s mean is denoted by μ¯X, as it is the mean of the sampling means. This distribution has some very interesting properties, which will later help you estimate 
  the sampling error. 
- The standard deviation of our sampling distribution is (StandardDeviation)2=∑[(Xi−¯X)^2]/n. Note that you would divide by n and not n-1 as you have the data for all 100 entries of the 
  distribution and you don't need to sample the distribution.
- The sampling distribution has some interesting properties that will later help you estimate the error in your samples
- The distribution with the higher sample size (n=20) would have a lower standard deviation(narrow curve) than (n=5) which has higher std deviation (wide curve)
- So, there are two important properties for a sampling distribution of the mean:Sampling distribution’s mean (μ¯X) = Population mean (μ)
  Sampling distribution’s standard deviation (Standard error) = σ/√n, where σ is the population’s standard deviation and n is the sample size.
- The central limit theorem says that, for any kind of data, provided a high number of samplings has been taken, the following properties hold true:
- The sampling distribution, which is basically the distribution of sampling means of samples, has some interesting properties which are collectively called the central limit theorem, 
  which states that no matter how the original population is distributed, the sampling distribution will follow these three properties:
	Sampling distribution’s mean (μ¯X) = Population mean (μ)
	Sampling distribution’s standard deviation (Standard error) = σ/√n
	For n > 30, the sampling distribution becomes a normal distribution 
- we found the mean commute time of 30,000 employees of an office by taking a small sample of 100 employees and finding their mean commute time. This sample’s mean 
  was X-bar = 36.6 minutes and its standard deviation was S = 10 minutes.
- We then said that this sample mean cannot be taken as the population mean, as there might be some errors in the sampling process. However, we can say that the population mean, 
  i.e. daily commute time of all 30,000 employees X-bar = 36.6 (sample mean) +- some margin of error.
- Now, you may be thinking that you can use the standard error for the margin of error. But, although the standard error provides a good estimate of this margin of error, 
  you cannot use it in place of the margin of error. To understand why, and how you would find the margin of error in that case, let's move on to the next lecture, where we will use 
  CLT (central limit theorem) to find the aforementioned margin of error.
- For example, for an office of 30,000 employees, we wanted to find the average commute time. So, instead of asking all employees, we asked only 100 of them and collected the data. 
  The mean = 36.6 minutes and the standard deviation = 10 minutes.
- However, it would not be fair to infer that the population mean is exactly equal to the sample mean. This is because the flaws of the sampling process must have led to some error. 
  Hence, the sample mean’s value has to be reported with some margin of error.
- solve numberic problem ref- Central Limit Theorem- practice question part 1
- So, to summarise, let’s say you have a sample with sample size n, mean \bar{X} and standard deviation S. Now, the y% confidence interval 
 (i.e. the confidence interval corresponding to y% confidence level) for \mu would be given by the range: confindence interval = [(X-bar+(Z*S/sqrt(n))),X-bar-(Z*S/sqrt(n))]
- where, Z* is the Z-score associated with a y% confidence level. In other words, the population mean and sample mean differ by a margin of error given by (Z*S/sqrt(n))
- Confidence level	Z*
   90%			+-1.65
   95%			+-1.96
   99%			+-2.58
- At this point, it is important to address a very common misconception. Sampling distributions are just a theoretical exercise and you’re not actually expected to make one in real life. 
  If you want to estimate the population mean, you will just take a sample. You will not create an entire sampling distribution.
- You must be wondering — if this is the case, then why did you study sampling distributions? To understand the reason for this, let's go through the actual process of sampling. Recall
  that you are doing sampling because you want to find the population mean, albeit in the form of an interval. The three steps to follow are as follows:
	First, take a sample of size n
	Then, find the mean and standard deviation S of this sample
	Now, you can say that for y% confidence level, the confidence interval for the population mean \mu, 
- you cannot finish step 3 without CLT. CLT lets you assume that the sample mean would be normally distributed, with mean and standard deviation  Using this assumption, it becomes 
  possible to find many things such as margin of error, confidence interval, etc.
- We estimated the mean commute time of 30,000 employees of an office, by taking a sample of 100 employees, finding their mean commute time, and estimating based on that value. 
  Specifically, you were given a sample with sample mean X = 36.6 minutes, and sample standard deviation S = 10 minutes.
- Using CLT, you concluded that the sampling distribution for mean commute time would have: 
  Mean = μ {unknown}
  Standard error = σ/√n≈S/√n=10/√100=1
  Since n(100) > 30, the sampling distribution is a normal distribution
- Using these properties, you were able to claim that the probability that the population mean μ lies between 34.6 (36.6-2) and 38.6 (36.6+2), is 95.4%.
- Then, you learnt some terminology related to the claim:
	The probability associated with the claim is called confidence level (here, it is 95.4%)
	The maximum error made in sample mean is called margin of error (here, it is 2 minutes)
	Final interval of values is called confidence interval [here, it is the range (34.6, 38.6)]
- Then, you generalised the whole process. Let’s say you have a sample with sample size n, mean \\bar{X} and standard deviation S. You learnt that the y% confidence interval 
  (i.e. confidence interval corresponding to y% confidence level) for \\mu will be given by the range:
- Confidence interval = [(X-bar+(Z*S/sqrt(n))),X-bar-(Z*S/sqrt(n))]
  Where, Z* is the Z-score associated with a y% confidence level.

- Hypothesis Testing:
- The statistical analyses learnt in Inferential Statistics enable you try to make inferences about the population mean from the sample data when you have no idea of the population mean.
  However, sometimes you have some starting assumption about the population mean and you want to confirm those assumptions using the sample data. It is here that hypothesis testing comes
  into the picture.
- Inferential statistics: Making inferences about the population using the sample data
- Now, these methods help you formulate a basic idea or conclusion about the population. Such assumptions are called “hypotheses”. But how do you really confirm these conclusions or 
  hypotheses?
- Difference between inferential statistics and hypothesis testing.
	Inferential statistics is used to find some population parameter (mostly population mean) when you have no initial number to start with. So, you start with the sampling activity
	and find out the sample mean. Then, you estimate the population mean from the sample mean using the confidence interval.
	Hypothesis testing is used to confirm your conclusion (or hypothesis) about the population parameter (which you know from EDA or your intuition). Through hypothesis testing, you 
	can determine whether there is enough evidence to conclude if the hypothesis about the population parameter is true or not.
- Hypothesis Testing starts with the formulation of these two hypotheses:
	Null hypothesis (H₀): The status quo
	Alternate hypothesis (H₁): The challenge to the status quo
- Please note than you can only fail to reject the null hypothesis, you can never accept the null hypothesis.
- Both the null and alternate hypotheses can’t be true at the same time. Only one of them will be true.
- The first step of hypothesis testing is the formulation of the null and alternate hypotheses for a given situation.
- If your claim statement has words like “at least”, “at most”, “less than”, or “greater than”, you cannot formulate the null hypothesis just from the claim statement 
  (because it’s not necessary that the claim is always about the status quo).
- You can use the following rule to formulate the null and alternate hypotheses:
- The null hypothesis always has the following signs:  =  OR   ≤   OR    ≥
- The alternate hypothesis always has the following signs:  ≠   OR  >   OR    <
- The null hypothesis is always formulated by either = or ≤ or ≥ whereas the alternate hypothesis is formulated by ≠ or > or <. In this case, the average time taken was greater than or 
  equal to 35 minutes. So, that becomes the null hypothesis. Less than 35 minutes becomes the alternate hypothesis.
- To summarize this, you cannot decide the status quo or formulate the null hypotheses from the claim statement, you need to take care of signs in writing the null hypothesis. 
  Null Hypothesis never contains ≠ or > or < signs. It always has to be formulated using = or ≤ or ≥ signs. 
- Once you have formulated the null and alternate hypotheses make the decision to either reject or fail to reject the null hypothesis 
- Critical region is the area which gives statistical significance to reject the null hypothesis.
- Once you have formulated the null and alternate hypotheses, let’s turn our attention to the most important step of hypothesis testing — making the decision to either reject or fail to 
  reject the null hypothesis
- So, you learnt about what critical values are and how your decision to reject or fail to reject the null hypothesis is based on the critical values and the position of the sample mean
  on the distribution.
- how the position of the critical region changes with the different types of null and alternate hypotheses.
- The formulation of the null and alternate hypotheses determines the type of the test and the position of the critical regions in the normal distribution.
- You can tell the type of the test and the position of the critical region on the basis of the ‘sign’ in the alternate hypothesis.
-  ≠ in H₁    →   Two-tailed test        →     Rejection region on both sides of distribution
   < in H₁    →   Lower-tailed test     →     Rejection region on left side of distribution
   > in H₁    →   Upper-tailed test     →     Rejection region on right side of distribution
- The average commute time for an UpGrad employee to and from office is at least 35 minutes.If this hypothesis has to be tested, select the type of the test and the location of the 
  critical region. = Lower-tailed test, with the rejection region on the left side. For this situation, the hypotheses would be formulated as H₀: μ ≥ 35 minutes and H₁: μ < 35 minutes.
                     As < sign is used in alternate hypothesis, it would be a lower-tailed test and the rejection region would be on the left side of the distribution.
- Now, let’s learn how to find the critical values for the critical region in the distribution and make the final decision of rejecting or failing to reject the null hypothesis.
- Before you proceed with finding the Zc and finally the critical values, let’s revise the steps performed in this method till now.
- First, you define a new quantity called α, which is also known as the significance level for the test. It refers to the proportion of the sample mean lying in the critical region. 
  For this test, α is taken as 0.05 (or 5%).
- Then, you calculate the cumulative probability of UCV from the value of α, which is further used to find the z-critical value (Zc) for UCV.
- After formulating the hypothesis, the steps you have to follow to make a decision using the critical value method are as follows:
- Calculate the value of Zċ from the given value of α (significance level). Take it a 5% if not specified in the problem.
- Calculate the critical values (UCV and LCV) from the value of Zċ.
- Make the decision on the basis of the value of the sample mean x with respect to the critical values (UCV AND LCV).
- Let’s solve the following problem stepwise to consolidate your learning on how to make a decision about any hypothesis.
- A manufacturer claims that the average life of its product is 36 months. An auditor selects a sample of 49 units of the product, and calculates the average life to be 34.5 months. 
  The population standard deviation is 4 months. Test the manufacturer’s claim at 3% significance level using the critical value method.
- First, you need to formulate the hypotheses for this two-tailed test, which would be:
-  H₀:μ = 36 months and H₁: μ ≠ 36 months
- find the critical values and make a decision.
- 1st step: Calculate the value of Zc from the given value of α (significance level). Calculate the z-critical score for the two-tailed test at 3% significance level. = 2.17
- 2nd step: Calculate the critical values (UCV and LCV) from the value of Zc.  Find out the UCV and LCV values for Zc = 2.17. = UCV = 37.24 and LCV = 34.76
- 3rd step: Make the decision on the basis of the value of the sample mean x with respect to the critical values (UCV AND LCV).  What would be the result of this hypothesis test?
	Reject the null hypothesis

- Do one numericl problem at ref-Concepts of Hypothysis testing 1- Critical valume method example.

- you learnt how to formulate the hypotheses for any situation and then make a decision through the critical value method. In that method, you learnt to find the critical values and 
  then compare the sample mean with the critical values to make your decision or rejecting or failing to reject the null hypothesis.
- There are various methods similar to the critical value method to statistically make your decision about the hypothesis. In this session, you will study one such method, which is called 
  the p-value method. This is an important method and is used more frequently in the industry.
- The p-value as the probability of the null hypothesis being accepted (or more aptly, not being rejected). This statement is not technically correct (or formal) definition of p-value, 
  but it is used for better understanding of the p-value.
- Higher the p-value, higher is the probability of failing to reject a null hypothesis. On the other hand, lower the p-value, higher is the probability of the null hypothesis being rejected.
- After formulating the null and alternate hypotheses, the steps to follow in order to make a decision using the p-value method are as follows:
	Calculate the value of z-score for the sample mean point on the distribution
	Calculate the p-value from the cumulative probability for the given z-score using the z-table
- Make a decision on the basis of the p-value (multiply it by 2 for a two-tailed test) with respect to the given value of α (significance value).
- To find the correct p-value from the z-score, first find the cumulative probability by simply looking at the z-table, which gives you the area under the curve till that point.
- Situation 1: The sample mean is on the right side of the distribution mean (the z-score is positive)
- Cumulative probability of sample point = 0.9987
	For one-tailed test  →    p = 1 - 0.9987 = 0.0013
	For two-tailed test  →    p = 2 (1 - 0.9987) = 2 * 0.0013 = 0.0026
- Situation 2: The sample mean is on the left side of the distribution mean (the z-score is negative)
- Cumulative probability of sample point = 0.0013
	For one-tailed test  →    p = 0.0013
	For two-tailed test  →    p = 2 * 0.0013 = 0.0026 

- Procedure to make hypothesis decesion using 
- Question - You are working as a data analyst at an auditing firm. A manufacturer claims that the average life of its product is 36 months. An auditor selects a sample of 49 units of 
  the product, and calculates the average life to be 34.5 months. The population standard deviation is 4 months. Test the manufacturer’s claim at 3% significance level.
- Answer- First, formulate the hypotheses for this two-tailed test, which would be:
	H₀: μ = 36 months and H₁: μ ≠ 36 months

1 . Critical Value Method.
Step 1: Calculate the value of Zċ from the given value of α (significance level, here it is 3%). Take it a 5% if not specified in the problem - z-score=
Step 2: Calculate the critical values (UCV and LCV) from the value of Zċ. UCV and LCV depends on the null hypothesis. Critical value= 
Step 3: Make the decision on the basis of the value of the sample mean x (=34.5)with respect to the critical values (UCV AND LCV). = 

2 . P-value method.
Step 1: Calculate the value of z-score for the sample mean point on the distribution. Calculate z-score for sample mean (x) = 34.5 months. - z-score= -2.62
Step 2: Calculate the p-value from the cumulative probability for the given z-score using the z-table. Find out the p-value for the z-score of -2.62 (corresponding to the sample mean of 
       34.5 months). Hint: The sample mean is on the left side of the distribution and it is a two-tailed test. - p-value=0.0088
Step 3: Make the decision on the basis of the p-value with respect to the given value of α (significance value). What would be the result of this hypothesis test? 
      - Reject the null hypothesis

- Types of Errors:
- While doing hypothesis testing, there is always the possibility of making the wrong decision about your hypothesis. These instances of a wrong decision being made are referred to as
  errors. Let’s learn about the different types of errors during hypothesis testing.
- There are two types of errors that can result during the hypothesis testing process — type-I error and type-II error.
	Type 1 error : Rejecting true null hypothesis
	Type 2 error : failing to reject false null hypothhesis 
- A type I-error represented by α occurs when you reject a true null hypothesis.
- A type-II error represented by β occurs when you fail to reject a false null hypothesis.
- The power of any hypothesis test is defined by 1 - β. 
- If go back to the analogy of the criminal trial example, you would find that the probability of making a type-I error would be more if the jury convicts the accused even on less 
  substantial evidence. The probability of a type-I error can be reduced if the jury adopts more stringent criteria to convict an accused party.
- However, reducing the probability of a type-I error may increase the probability of making a type-II error. If the jury becomes very liberal in acquitting the people on trial, there 
  would be a higher probability that an actual criminal is able to walk free.
- Type-I error occurs when the null hypothesis is rejected when it is in fact correct
- Type II error occurs when the null hypothesis is not rejected when it is in fact incorrect
- Suppose the null hypothesis is that a particular new process is as good as or better than the old one. A type-I error is to conclude that: The old process is better than the new one,
  when it is not.

- Industrial demonstration of Hypothesis testing:
- In the previous sessions, you learnt about the basic concepts of hypothesis testing and learnt how to statistically test the inferences made from inferential statistics or the insights
  generated during EDA. You learnt how to formulate the hypotheses and then make a decision through either the critical value method or the p-value method.
- As an analyst in the industry, when you would use hypothesis testing, the standard deviation of the population would be unknown most of the times, and for sample size <30 we go 
  for T-distribution
- A t-distribution is also referred to as Student’s T distribution. A t-distribution is similar to the normal distribution in many cases; for example, it is symmetrical about its 
  central tendency. However, it is shorter than the normal distribution and has a flatter tail, which would eventually mean that it has a larger standard deviation.
- At a sample size beyond 30, the t-distribution becomes approximately equal to the normal distribution.
- The most important use of the t-distribution is that you can approximate the value of the standard deviation of the population (σ) from the sample standard deviation (s). However,
  as the sample size increases more than 30, the t-value tends to be equal to the z-value
- [Sample size >30] -->(yes)     [Normal Distibution]
      -(no)			-
      -				-
    [if we know ----(yes)--------
     stand.dev]
       -(no)
       -
      [t-distribution]
- Let’s look at how the method of making a decision changes if you are using the sample’s standard deviation instead of the population’s. If you recall the critical value method, 
  the first step is as follows:
	Calculate the value of Zc from the given value of α (significance level). Take it as 5% if not specified in the problem.
- So, to find Zc, you would use the t-table instead of the z-table. The t-table contains values of Zc for a given degree of freedom and value of α (significance level). Zc, in this case, 
  can also be called as t-statistic (critical).

- 3. T-Distribution
  Step 1: You are given the standard deviation of a sample of size 25 for a two-tailed hypothesis test of a significance level of 5%. Use the t-table given above to find the value of Zc.
	  degree of freedom = n-1 = 25-1, Zc=2.064
  Step 2: using Zc finding the critical value (steps same as normal distribution) and making the hypothesi decesion.

- Using the the t-table to find the value of Zc for sample size = 32 and a significance level of 5%. If you use the z-table for the same, you would get the same value of Zc, since, 
  ****for sample size ≥ 30, the t-distribution is the same as the z-distribution.*****
- Practically you would not need to refer to the z-table or t-table when doing hypothesis testing in the industry. Going forward when you need to do hypothesis testing in demonstrations 
  of Excel or R, you would use the term t-test since that is mostly performed in the industry. All calculations and results of a t-test are same as the z-test whenever the 
  sample size ≥ 30. 

-Two-Sample Mean Test:
- Two-sample mean test - paired is used when your sample observations are from the same individual or object. During this test, you are testing the same subject twice. For example, if 
  you are testing a new drug, you would need to compare the sample before and after the drug is taken to see if the results are different
- Two-sample mean test - unpaired is used when your sample observations are independent. During this test, you are not testing the same subject twice. For example, if you are testing a 
  new drug, you would compare its effectiveness to that of the standard available drug. So, you would take a sample of patients who consumed the new drug and compare it with another 
  sample who consumed the standard drug.
- What can you conclude from this test about the two unpaired samples if you take a significance level of 5%? = There is some significant difference between the means of the two samples
   The null hypothesis in this test is that the means of the two samples are the same, and there is no significant difference between them. As the p-value (0.022) is less than 0.05 
   (value of α), you can reject the null hypothesis and conclude that there is some significant difference between the means of the two samples.

- Two-Sample Proportion Test:
- ne thing you should observe in these tests is that the data from the sample is always numeric in nature. But what would you do if the data is categorical in nature, i.e. 1 or 0;
  Yes or No, etc. Two-sample proportion test is used when your sample observations are categorical, with two categories. It could be True/False, 1/0, Yes/No, Male/Female, Success/Failure
  etc. Two-sample proportion test is used when your sample observations are categorical, with two categories. It could be True/False, 1/0, Yes/No, Male/Female, Success/Failure etc. 
- For example, if you are comparing the effectiveness of two drugs, you would define the desired outcome of the drug as the success. So, you would take a sample of patients who consumed 
  the new drug and record the number of successes and compare it with successes in another sample who consumed the standard drug. 

- A/B Testing Demonstration:
- A/B testing. A/B testing is a direct industry application of the two-sample proportion test sample you have just studied. 
- While developing an e-commerce website, there could be different opinions about the choices of various elements, such as the shape of buttons, the text on the call-to-action buttons,
  the colour of various UI elements, the copy on the website, or numerous other such things.
- Often, the choice of these elements is very subjective and is difficult to predict which option would perform better. To resolve such conflicts, you can use A/B testing. A/B testing 
  provides a way for you to test two different versions of the same element and see which one performs better.
- The two-sample proportion test is used when you want to compare the proportions of two different samples. 

- Hypothesis testing in Python:
-You can easily do hypothesis testing in Python by using stats from Scipy library.
import scipy
from scipy import stats

- 1-sample t-test: testing the value of a population mean
- To test, if the population mean of data is likely to be equal to a given value
- stats.ttest_1samp()
- stats.ttest_1samp(data['column'], x)
 #where x is the mean value you want to test

- 2-sample t-test: testing for difference across populations
- stats.ttest_ind()
- stats.ttest_ind(column_1,column_2)  
Paired tests: repeated measurements on the same individuals
- stats.ttest_rel()  
- stats.ttest_rel(column_1,column_2)  

- Different industry experts have different views on the applications and usability of hypothesis testing because of different reasons.

- To summarise, hypothesis testing still holds importance in the following two types of industries, even if all the data is available digitally:
- Manufacturing processes in the food, pharmaceuticals, chemicals industries, where it is not practically possible to gather information on the entire population.
- E-commerce, advertising and digital marketing companies, where the amount of data collected from various samples is so huge that analysing all data becomes very difficult without 
  having big data systems in place.

- Summary:
- T-distribution:
	A T-distribution is used whenever the standard deviation of the population is unknown
	The degrees of freedom of a T-distribution is equal to sample size n - 1
	For sample size ≥ 30, the T-distribution becomes the same as the normal distribution
	The output values and results of both t-test and z-test are same for sample size ≥ 30

- Two-sample mean test - paired:
	It is used when your sample observations are from the same individual or object
	During this test, you are testing the same subject twice

- Two-sample mean test - unpaired:
	During this test, you are not testing the same subject twice
	It is used when your sample observations are independent

- Two-sample proportion test:
	It is used when your sample observations are categorical, with two categories
	It could be True/False, 1/0, Yes/No, Male/Female, Success/Failure, etc. 

- A/B Testing:
	A/B testing is a direct industry application of the two-sample proportion test
	It is a widely used process in digital companies in the ecommerce, manufacturing and advertising domains
	It provides a way to test two different versions of the same element and see which one performs better



- Full outer join is used to merge the data sets such that, apart from keeping the common Employee IDs, it also includes the Employee IDs unique to both the data sets.
- rating.drop_duplicates() ---drops duplicate rows
- ordered and unordered categorical variables -
- Ordered ones have some kind of ordering. Some examples are
	Salary = High-Medium-low
	Month = Jan-Feb-Mar etc.
- Unordered ones do not have the notion of high-low, more-less etc. Example:
	Type of loan taken by a person = home, personal, auto etc.
	Organisation of a person = Sales, marketing, HR etc.
- The other most common type is quantitative variables. These are simply numeric variables which can be added up, multiplied, divided etc. 
  For example, salary, number of bank accounts, runs scored by a batsman, the mileage of a car etc.
- The objective of using a log scale is to make the plot readable by changing the scale.(Plotting on a log scale compresses the values to a smaller scale which makes the plot easy to read.)
- whenever you have a continuous or an ordered categorical variable, make sure you plot a histogram or a bar chart and observe any unexpected trends in it.
- standard deviation will be influenced by outliers while the interquartile difference will simply ignore them.
- Interquartile difference is a much better metric than standard deviation if there are outliers in the data.
- 


*****************************************************************************************************************************************************************
# Linear Regression
- Properties of LR:
	1) Linear relationship between X and y.
	2) Normal distribution of error terms. (histplot(res))
	3) Independence of error terms. (sns.regplot)(plot y,error)
	4) Constant variance of error terms.(Homoscedasity) (plot x,y)

- The sum of residuals in LR model must be zero.
- F statistics must be high --> leads to Prob(F) low --> model is significant.
- Correlation: sign will tell us the praportionality ie(realted directly or inversly) | Magnitude will tell how the data points have hugged the line.
  (Correlation has nothing to do with slope of the line)
  (The correlation coefficient specifies how strong is the relationship between two variables.)
- The R-squared will always either increase or remain the same when you add more variables. Because you already have the predictive power 
  of the previous variable so the R-squared value can definitely not go down. And a new variable, no matter how insignificant it might be, 
  cannot decrease the value of R-squared.

- Multicollinearity: Multicollinearity refers to the phenomenon of having related predictor variables in the input dataset. 
  In simple terms, in a model which has been built using several independent variables, some of these variables might be interrelated, 
  due to which the presence of that variable in the model is redundant. You drop some of these related independent variables as a way of dealing with multicollinearity.

- Multicollinearity affects:

Interpretation:
Does “change in Y, when all others are held constant” apply?
Inference: 
Coefficients swing wildly, signs can invert
p-values are, therefore, not reliable

- Two basic ways of dealing with multicollinearity
 1) Looking at pairwise correlations
 2) Checking the Variance Inflation Factor (VIF) (VIF calculates how well one independent variable is explained by all the other independent variables combined)

The common heuristic we follow for the VIF values is:

> 10:  Definitely high VIF value and the variable should be eliminated.
> 5:  Can be okay, but it is worth inspecting.
< 5: Good VIF value. No need to eliminate this variable.

- Some methods that can be used to deal with multicollinearity are:

 1) Dropping variables
	Drop the variable which is highly correlated with others
	Pick the business interpretable variable
 2) Create new variable using the interactions of the older variables
	Add interaction features, i.e. features derived using some of the original features
 3) Variable transformations
	Principal Component Analysis (covered in a later module)

Note: It is important to note that scaling just affects the coefficients and none of the other parameters like t-statistic, F-statistic, p-values, R-squared, etc
Note: R^2 = (correlation)^2

**************************************************************************************************************************************************************************************
# Naive Bayes:
- Naive Bayes is a probabilistic classifier which returns the probability of a test point belonging to a class rather than the label of the test point.
- You saw how Laplace smoothing helped solve the zero probability problem.
- Bernoulli Naive Bayes is concerned only with whether the word is present or not in a document, whereas Multinomial Naive Bayes counts the no. of occurrences of the words as well.
- Sensitivity implies that out of all the actual Spam's, how many of them were correctly predicted by the model as Spam. 
- Specificity implies that out of all the genuine SMS's, how many of them were correctly predicted as legitimate by the model.
- Naïve Bayes follows an assumption that the variables are conditionally independent given the class i.e. P(X = convex,smooth | C= edible) can be written as 
  P(X=smooth | C=edible)*P(X=convex | C=edible).
- Hence, the name “Naïve” 
- You have been using 3 terms: P(Class = edible / poisonous), P(X | Class) and P(Class | X). 
- Bayesian classification is based on the principle that ‘you combine your prior knowledge or beliefs about a population with the case specific information to get the actual (posterior) 
  probability’.
- If the prior is neutral (50% are edible), then the likelihood may largely decide the outcome.
- P(Class = edible | X) is the posterior probability, It is the outcome which combines prior beliefs and case-specific information. It is a balanced outcome of the prior and the likelihood.
- Sensitivity (rate of true positives) and Specificity (rate of true negatives).
- If we have a vast vocabulary of let's say 2000 or 3000 words, using a Sparse Matrix for Naive Bayes classifier will not be efficient. This is because a Sparse Matric has so many zeros
  and storing so many zeros in the memory is a waste of memory. Now the way to get rid of these is know as a Compressed Sparse Row format.
- ROC - Receiver Operating Characteristic Curve (x-axis= False Positive Rate ; y-axis= True positive Rate. (worst:0.5 ; best:1)(ROC Curve is the tradeoff between sensitivity and specificty)
- ROC Curves are used to see how well your classifier can separate positive and negative examples and to identify the best threshold for separating them.
- The area under the ROC Curve shows how far the curve from the base line. For the baseline it's 0.5, and for the perfect classifier it's 1. In our case the AUC obtained was 99% which 
  is very good. 
- Binarization of a feature vector: Converting all non-zero word count of a feature vector to 1 and leaving zero counts as it is
- While trying to calculate the likelihood of a test document for a given class, it is possible that there exist certain words which although are a part of the dictionary but don't 
  appear in the training documents of that class like the word pepsi does not appear in documents of hot class. Then, the probability of that word for that class becomes zero
  ( P(pepsi|hot) =0 )  and it makes the complete likelihood term zero. This is called the zero-probability problem.
- To counter this problem, a ‘1’ is added to the total of every word count of all the words of the dictionary for that class. This increases the total word count for that class by 
  the length of the dictionary. This technique is called Laplace Smoothing.
- If we are adding 1 to one of the class then also add 1 to the other class as well even if there is no 0 likelihood in the other class.(Laplace smoothing should be applied for 
  all the classes at a time)
- Probability of a Female student being a DL student | means P(DL|F) 
- To improve the accuracy of the model check if the stop words are removed. Also remove the very rarely(~3%) and mostly(~80%) occuring words from the feature vector.

*************************************************************************************************************************************************************************************
# K-Fold Cross Validation
  In a k-fold cross validation (k-fold CV), you divide the entire data into k-folds, and then build k models. Thus, if k=5, you will build five models each of which will use one fold 
  as the test data and the rest 80% as the training data. Note that if k=5, each fold will be 1/5 or 20% the size of the data. Thus, you'll have 5 models, and then you can measure the
  'average accuracy of the 5 models' which is likely to be a better representation of the model's true accuracy.


***********************************************************************************************************************************************************************************
# Linear Regression

- OLS - Ordinary Least Square : the best-fit line is obtained by minimising a quantity called Residual Sum of Squares (RSS) 
- Gradient Descent : Gradient Descent is an optimisation algorithm which optimises the objective function (for linear regression it's cost function) to reach to the optimal solution.
- Apart from R², there is one more quantity named RSE (Residual Square Error) which is linked to RSS.
- RSE = SQRT(RSS/df), where df = degree of freedom = n-2, where n = number of data points.
- The equation of the best fit regression line Y = β₀ + β₁X can be found by minimising the cost function (RSS in this case, using the Ordinary Least Squares method) which is done 
  using the following two methods:
	* Differentiation
	* Gradient descent method
- The strength of a linear regression model is mainly explained by R²,  where R² = 1 - (RSS / TSS)
	RSS: Residual Sum of Squares
	TSS: Total Sum of Squares
- The value of the correlation coefficient always lies between -1 and 1, where a negative value implies a negative correlation, a positive value shows a positive correlation, and a 
  zero value shows no correlation.
- The value of R-squared lies between 0 and 1, where 1 implies that all the variance in the data is being explained by the model, and 0 implies that none of the variance values is being 
  explained by the model. Obviously, it is very difficult to achieve either of the extreme values.
- If the relationship between X and y is perfectly linear in both cases. Their slopes are 1 and 0.5. But since all the points lie on the straight line, the correlation coefficient would 
  be 1. 
- The slope of the straight line does not determine the correlation between the variables.
- If the correlation between y and X is -0.92, The absolute value of the correlated coefficient is very high. The negative sign just implies that X and y are negatively correlated. 
  Hence, they have a very strong negative correlation.
- MSS is the model sum of squares (also known as ESS, or explained sum of squares), R2 = MSS/TSS = (TSS − RSS)/TSS
- The effect of the error terms not being homoscedastic in nature is Even if we fit a line through the data, we cannot make inferences about the model. The parameters used to make 
  inferences will become highly unreliable.
- You start by saying that β1 is not significant, i.e. there is no relationship between X and y. So in order to perform the hypothesis test, we first propose the null hypothesis that β1
  is 0. And the alternative hypothesis thus becomes β1 is not zero. Null Hypothesis (H0): β1=0 | Alternate Hypothesis (HA): β1≠0.
- OLS stands for Ordinary Least Squares, which is the method that 'statsmodels' use to fit the line. 
- You use the command 'add_constant' so that statsmodels also fits an intercept. If you don't use this command, it will fit a line passing through the origin by default.
- F-Statistics tells you whether the overall model fit is significant or not.
- If the 'Prob (F-statistic)' is less than 0.05, you can conclude that the overall model fit is significant. If it is greater than 0.05, you might need to review your model as the fit 
  might be by chance, i.e. the line may have just luckily fit the data.
- R-squared value tells you exactly how much variance in the data has been explained by the model.
- You just plot a histogram of the error terms to check whether they are normally distributed. And another assumption was that the error terms should be independent of each other.
  Again for this, you need to plot the error terms, this time with either of X or y to check for any patterns
- RMSE (Root Mean Squared Error) is a metric that tells you the deviation of the predicted values by a model from the actual observed values. So, since it is a sort of error term, 
  it is better to have a low RMSE.
- In SKLearn lm.coeff_ gives you the value of β1 which is the slope of the fitted line.
- Assumptions of simple linear regression:
	Linear relationship between X and y.
	Normal distribution of error terms.
	Independence of error terms.
	Constant variance of error terms.
- Hypothesis testing in linear regression
- To determine the significance of beta coefficients.H0: β1=0; HA: β1≠0. T-test on the beta coefficient. t-score=βi/SE(βi).
- Residual Analysis: 
	Histogram of the error terms to check normality.
	Plot of the error terms with X or y to check independence.
- Maximum Likelihood:  method to find the best fit line for Logistic regression.
- Least Square Error:  method to find the best fit line for linear regression.
- The sum of residuals should be equal to zero in linear regression.
- We can build linear regression model by using SciPy also.
- By default, statsmodels fits a line passing through the origin, i.e. it doesn't fit an intercept. Hence, you need to use the command 'add_constant' so that it also fits an intercept.
- The R-squared will always either increase or remain the same when you add more variables. Because you already have the predictive power of the previous variable so the R-squared value 
  can definitely not go down. Add a new variable, no matter how insignificant it might be, cannot decrease the value of R-squared.
- The new aspects to consider when moving from simple to multiple linear regression are:
  Overfitting:
  	As you keep adding the variables, the model may become far too complex.
	It may end up memorising the training data and will fail to generalise.
  	A model is generally said to overfit when the training accuracy is high while the test accuracy is very low.
  Multicollinearity:
	Associations between predictor variables.
  Feature selection:
	Selecting the optimal set from a pool of given features, many of which might be redundant becomes an important task.

- The predictive power given by the R-squared value is not affected because even though you might have redundant variables in your model, they would play no role in affecting the R-squared. 
  Recall the thought experiment that Rahim had conducted in one of the lectures. So suppose you have two variables, X1 and X2 which are exactly the same. So using any of the following, 
  say, 10X1 or (4X1 + 6X2) will give you the same result. In the second case, even though you have increased one variable, the predictive power remains the same.
- Adjusted R2 adjusts the value of R2 such that a model with a larger number of variables is penalized
- Adjusted R-squared - The adjusted R-squared value increases only if the new term improves the model more than would be expected by chance.
- AIC, BIC - Various types of criteria used for automatic feature selection 
- Overfitting is the condition wherein the model is so complex that it ends up memorising almost all the data points on the train set. Hence, this condition is more probable if the number 
  of data points is less since the model passing through almost every point becomes easier.
- Automated approach for linear regression:
	Recursive Feature Elimination
	Stepwise Selection using AIC
	Regularisation
- If the variable is being described well by the rest of the feature variables, it means it has a high VIF meaning it is redundant in the presence of the other variables.
- The advantage of Standardisation over the other is that it doesn't compress the data between a particular range as in Min-Max scaling. This is useful, especially if there are extreme 
  data point (outlier). standardisation = ((X-mean)/Std.deviation). min-max = (X-Xmin)/(Xmax-Xmin)
- Pearson correlation coefficient between 2 variables might be zero even when they have a relationship between them.If the correlation coefficient is zero, it just means that that they 
  don’t move together. We can take examples like y=|x| or y=x^2.
- The slope of the regression line will change due to outliers in most of the cases. So Linear Regression is sensitive to outliers.
- RMSE tells us how close the actual data points are to predictions made by the model
- Two methods to obtain our model coefficients by minimising RSS:
	1) Using an optimisation algorithm, such as gradient descent:
		To perform gradient descent, we initialise the weights to some value (e.g., all zeros) and repeatedly adjust them in the direction that decreases the cost function. 
                We repeat this procedure until the betas converge, or stop changing much. Ultimately, the final betas would be close to the optimum.
	2) Using normal equations to solve for model coefficients:
		In normal equations, we calculate the model coefficients b0 and b1 at which our cost function, i.e., RSS, is minimum by using derivatives. In order to do this:
			Take the derivative of the cost function w.r.t. b0 and b1,
			Set each equation to 0 and
			Solve the two equations to get the best values for the parameters b0 and b1.
- We can use this solution (using matrix form) to find the coefficients for simple & multiple linear regression as: [β]=(([X]'[X])^-1)([X]'[Y])

- Identifying nonlinearity in data for simple linear regression and multiple linear regression: 
	For Simple Linear Regression: 
		Plot the independent variable against the dependent variable to check for nonlinear patterns.
	For Multiple Linear Regression, since there are multiple predictors, we, instead, plot the residuals versus the predicted values, (^yi). 
		Ideally, the residual plot will show no observable pattern. In case a pattern is observed, it may indicate a problem with some aspect of the linear model. 
                Apart from that:
			Residuals should be randomly scattered around 0.
			The spread of the residuals should be constant.
			There should be no outliers in the data
	If nonlinearity is present, then we may need to plot each predictor against the residuals to identify which predictor is nonlinear.
- Three methods to handle nonlinear data:
	Polynomial regression
	Data transformation
	Nonlinear regression
- Polynomial regression can be considered an extension of multiple linear regression and, hence, we can use the same technique used in multiple linear regression to estimate the model 
  coefficients for polynomial regression. 
- On inspection of the relationship between one predictor variable (a) and the response variable (y), you identify that the two have a cubic relationship. hence we include a,a^2,a^3. 
  (Since this is a cubic fit, we need to include third degree of the predictor. In polynomial regression, we need to include the lower degree polynomials in the model as well. 
  Hence, we include all three predictors as mentioned in the answer.)
- Data Transformation:
	Here in order to use linear model we need to transform our data so tat it follows linearity. we first plot target v/s predictor and if we get log like curve then we do replace 
  the predictor variable with log(predictor variable) which will then convert their relationship into linear one. hence now we can use our linear regression model (Also now all the 
  assumptions of linear regression must follow)
- [Residual v/s predictor plot will tell us if the data holds good for linear model in case of multiple predictor variables],[in case of only 1 predictor variable simply plot x v/s y]
- When we transform the response variable, we will change both its errors and variance. Hence, we should transform the response variable only if the error terms are not normal or if the 
  residuals exhibit non-constant variance, as seen in the residual plots.
  Transformations that can be applied on the response variable can include natural log, square root or inverse, i.e., ln(y), √y or 1/y, respectively.
  However, if nonlinear trends are observed in the residual plots, then we should first try to transform the predictor variable(s). Despite these guidelines, the transformations may not 
  always work.

- Regularisation:
- To reduce the complexity of the model, we can either reduce some of the coefficients selectively and/or remove variables that do not contribute much to the final result. 
  To achieve this, we will use regularization.
- Regularization helps with managing model complexity by essentially shrinking the model coefficient estimates towards 0. This discourages the model from becoming too complex, 
  thus avoiding the risk of overfitting.
- we use regularization because we want our models to work well with unseen data, without missing out on identifying underlying patterns in the data. For this, we are willing to make a 
  compromise by allowing a little bias for a significant reduction in variance. We also understood that the more extreme the values of the model coefficients are, the higher are the 
  chances of model overfitting. Regularization prevents this by shrinking the coefficients towards 0. (by introducing penalty on the cost function)
- One point to keep in mind is that we need to standardise the data whenever working with Ridge regression.
- Ridge regression has a particular advantage over OLS when the OLS estimates have high variance, i.e., when they overfit. Regularization can significantly reduce model variance while 
  not increasing bias much. 
- The tuning parameter lambda helps us determine how much we wish to regularize the model. The higher the value of lambda, the lower the value of the model coefficients, and more is 
  the regularization. 
- Choosing the right lambda is crucial so as to reduce only the variance in the model, without compromising much on identifying the underlying patterns, i.e., the bias.  
- Ridge regression does have one obvious disadvantage. It would include all the predictors in the final model. This may not affect the accuracy of the predictions but can make model 
  interpretation challenging when the number of predictors is very large.
- Ridge regression model coefficients can go very close to 0 but not become 0.
- With Lasso, the penalty pushes some of the coefficient estimates to be exactly 0, provided the tuning parameter, λ, is large enough.Hence, Lasso performs feature selection
- Also, just like with Ridge regression, standardisation of variables is necessary for Lasso as well.
- Generally, Lasso should perform better in situations where only a few among all the predictors that are used to build our model have a significant influence on the response variable. 
  So, feature selection, which removes the unrelated variables, should help. 
  But Ridge should do better when all the variables have almost the same influence on the response variable. 
- It is not the case that one of the techniques always performs better than the other – the choice would depend upon the data that is used for modelling.
- Ridge Regression is a technique for analysing multiple linear regression data that suffer from multicollinearity.
- Lasso Regression comes with huge computational cost. (Ridge has significantly lower computational costs since the matrix is invertible and easily solvable)
- The ‘linear’ in linear regression does not stand for the relation between the target variable and the predictor variable. In fact, it stands for the coefficients of the predictor 
  terms in the linear regression solution. (betas should be linear)
- In the Best Subset Selection algorithm, we start with 0 features, i.e. a null model M0 with no features. Now, as we increase the number of features, we consider every model that has 
  all combinations of a certain number of features. and select a model which results in the least RSS (or largest R2).
  This gives us a model Md with d features. We continue this iteration by increasing the value of d by one till you reach d is equal to the number of features in the dataset and find 
  the models M0, M1, M2,....., Mp. Out of all these models M0, M1, M2,....., Mp, select the best one, as measured by measures such as Cp, AIC, BIC, Adjusted R2 or 
  mean cross-validated error.

*****************************************************************************************************************************************************************************************
# Logistic Regression

- In order to find the best-fit sigmoid curve, you need to vary β0 and β1 until you get the combination of beta values that maximises the likelihood.
- Likelihood = [(1-P1)*(1-P2)...Non Diabetic]*[(P5)*(P6)....Diabetic]
- Maximum Likelihood:  method to find the best fit line for Logistic regression.
- This process, where you vary the betas until you find the best fit curve for the probability of diabetes, is called logistic regression.
- Suppose you are discussing sugar levels and the probability they correspond to. While talking about 4 patients with sugar levels of 180, 200, 220 and 240, you will not be able to 
  intuitively understand the relationship between their probabilities (10%, 28%, 58%, 83%). However, if you are talking about the log odds of these 4 patients, you know that their 
  log odds are in a linearly increasing pattern (-2.18, -0.92, 0.34, 1.60) and that the odds are in a multiplicatively increasing pattern (0.11, 0.40, 1.40, 4.95, increasing by a 
  factor of 3.55).
- Hence, many times, it makes more sense to present a logistic regression model’s results in terms of log odds or odds than to talk in terms of probability. This happens especially a 
  lot in industries like finance, banking, etc
- (P/1-P), indicate how much likelier a person is to have diabetes than to not have it. 
- For example, a person for whom the odds of having diabetes are equal to 3, is 3 times more likely to have diabetes than to not have it. In other words, P(Diabetes) = 3 * P(No diabetes).
- Basically, with every linear increase in x, the increase in odds is multiplicative. For example, in the diabetes case, after every increase of 11.5 in the value of x, the odds are 
  approximately doubled, i.e. they increase by a multiplicative factor of about 2.
- MULTIVARIATE:
- We Conduct feature selection for logistic regression using:
	Automated methods: RFE -Recursive Feature Elimination
	Manual methods: VIF and p-value check
- High value of P(>0.05) implies the variable is statistically insgnificant, VIF tells if the variable is redundent(>5).
- you definitely need to check the VIFs as well to further eliminate the redundant variables. Recall that VIF  calculates how well one independent variable is explained by all the other 
  independent variables combined.
- VIF(i)=1/(1-Ri^2)
- There is a measure known as F1-score which essentially combines both precision and recall. It is the basically the harmonic mean of precision and recall and its formula is given by: 
- F1-score = 2*[(precision*recall)/(precision+recall)]
- The logistic regression model separates two different classes using a line linearly. The sigmoid curve is only used to calculate class probabilities. The final classes are predicted 
  based on the cutoff chosen after building the model.


**************************************************************************************************************************************************************************************

# MODEL SELECTION
- The central issue in all of machine learning is how do we extrapolate learnings from a finite amount of available data to all possible inputs ‘of the same kind’?
- Occam’s razor does not suggest that a model should be unjustly simplified until no further simplification is possible. It suggests that when faced with a trade-off between a complex and
  a simple model, with all other things being roughly equal, you are better off choosing the simpler one. 
- A simpler model requires fewer training data points. This becomes extremely important because in many cases, one has to work with limited data points.
- A simple model is more robust and does not change significantly if the training data points undergo small changes.
- Overfitting is a phenomenon wherein a model becomes highly specific to the data on which it is trained and fails to generalise to other unseen data points in a larger domain.
- Variance refers to changes in the model as a whole when trained on a different data set.
- Regularization is the process of deliberately simplifying models to achieve the correct balance between keeping the model simple and not too naive.
- Pros: Logistic regression
	It is convenient for generating probability scores.
	Efficient implementation is available across different tools.
	The issue of multicollinearity can be countered with regularisation.
	It has widespread industry use.

	Decision trees
	Intuitive decision rules make it easy to interpret.
	Trees handle nonlinear features well.
	The variable interaction is taken into account.
	
	Support vector machines	
	SVMs can handle large feature space.
	These can handle nonlinear feature interaction.
	They do not rely on the entire dimensionality of the data for the transformation.

- Cons: Logistic regression
	It does not perform well when the features space is too large.	
	It does not perform well when there are a lot of categorical variables in the data.
	The nonlinear features have to be transformed to linear features in order to efficiently use them for a logistic model.
	It relies on entire data i.e. if there is even a small change in the data, the logistic model can change significantly.
	
	Decision trees
	Trees are highly biased towards the training set and overfit it more often than not.
	There is no meaningful probability score as the output.

	Support vector machines
	SVMs are not efficient in terms of computational cost when the number of observations is large.
	It is tricky and time-consuming to find the appropriate kernel for a given data.
- Decision trees are best suited for a dataset with a lot of categorical data because of the way in which node splitting is performed. Decision trees do not need the categorical 
  features to be converted into numeric features.
- CART (Classification and regression trees)
	It performs binary split at each node
	It can do regression and classification both
	It grows a large tree first, and then prunes it back to a small tree
	It may easily overfit unless the tree is pruned back
- CHAID (Chi Square Automatic Interaction Detection)
	It uses multiway splits by default 
	It is intended to work with categorical or descritized targets
	It uses truncation by making a node split only if a significance criterion is fulfilled
	It tries to prevent overfitting right from the start.

- You looked at the different applications of CART and CHAID trees. To put them in the form of an analogy, suppose you are working with the Indian cricket team, 
  and you want to predict whether the team will win a particular tournament or not. In this case, CART would be more preferable because it is more suitable for prediction tasks. 
  Whereas, if you want to look at the factors that are going to influence the win/loss of the team, then a CHAID tree would be more preferable.
- CART can only create binary trees (a maximum of two children for a node), and CHAID can create multiway trees (more than two children for a node).
- KPIs (Key Performance Indicators), i.e. features that play an important role in the problem at hand
- CHAID trees are suitable when you need to understand the driver KPIs, instead of predicting the class.
- Disadvantages of decision trees:
	Trees have a tendency to overfit the training data.
	Splitting with multiple linear decision boundaries that are perpendicular to the feature space is not always efficient.
	It is not possible to predict beyond the range of the response variable in the training data in a regression problem. Suppose you want to predict house prices using a 
        decision tree and the range of the the house price (response variable) is $5000 to $35000. While predicting, the output of the decision tree will always be within that range.

- Advantages of random forests:
	No need to prune the trees of a forest.
	Accuracy (implicit OOB error calculation) and variable importance generated automatically (Many people use random forest to calculate variable importance becoz it automatically 
        generates it)
	The OOB error can be calculated from the training data itself which gives a good estimate of the model performance on unseen data.
	It is hard for a random forest to overfit the training data.
	A random forest is not affected by outliers as much because of the aggregation strategy. 

- The limitations of a random forest are:
	Owing to their origin to decision trees, random forests have the same problem of not predicting beyond the range of the response variable in the training set.
	The extreme values are often not predicted because of the aggregation strategy. 
	To illustrate this, let’s take the house prices example, where the response variable is the price of a house. Suppose the range of the price variable is between $5000 and $35000. 
        You train the random forest and then make predictions. While making predictions for an expensive house, there will be some trees in the forest which predict the price of the house
        as $35000, but there will be other trees in the same forest with values close to $35000 but not exactly $35000. In the end, when the final price is decided by aggregating using 
        the mean of all the predictions of the trees of the forest, the predicted value will be close to the extreme value of $35000 but not exactly $35000. Unless all the trees of the 
        forest predict the house price to be $35000, this extreme value will not be predicted.
- start with a logistic regression model. Then, build a decision tree model. While building a decision tree, you should choose the appropriate method: CART for predicting and CHAID for 
  driver analysis. If you are not satisfied with the model performance mentioned so far, and you have sufficient time and resources in hand, then go ahead and build more complex models 
  like random forests and support vector machines.
- A decision tree generally does not perform well on a dataset with a lot of continuous variables.

*********************************************************************************************************************************************************************************************

# CLUSTERING:
- On the other hand, in unsupervised learning, you are not interested in prediction because you do not have a target or outcome variable. The objective is to discover interesting 
  patterns in the data, e.g. are there any subgroups or ‘clusters’ among the bank’s customers?
- In clustering, we group the data points into different categories based on the given set of attributes. There are no dependent and independent variables.
- Difference b/w Clutering and Segmentation: 
	Clustering is an Analytics Technique whereas Segmentation is a business problem. To do segmentation we use clustering technique.
- for successful segmentation, the segments formed must be stable. This means that the same person should not fall under different segments upon segmenting the data on the same criteria. 
  You also saw that segments should have intra-segment homogeneity and inter-segment heterogeneity.
- You saw that mainly 3 types of segmentation are used for customer segmentation:
	Behavioural segmentation: Segmentation is based on the actual patterns displayed by the consumer
	Attitudinal segmentation: Segmentation is based on the beliefs or the intents of people, which may not translate into similar action
	Demographic segmentation: Segmentation is based on the person’s profile and uses information such as age, gender, residence locality, income, etc.
- Recharge (mobile recharge multiple times in a month) is a behaviour which can be observed, opposed to attitude which resides in the mindset of the customer.
- the algorithm needs to find data points whose values are similar to each other and therefore these points would then belong to the same cluster. The method in which any clustering 
  algorithm goes about doing that is through the method of finding something called a “distance measure”. 
  The distance measure that is used in K-means clustering is called the Euclidean Distance measure.
- The Euclidean Distance between the 2 points is measured as follows: If there are 2 points X and Y having n dimensions:
	X=(X1,X2,X3,...Xn)	Y=(Y1,Y2,Y3,....Yn)
- Then the Euclidean Distance D is given as D =√[(X1−Y1)^2+(X2−Y2)^2+...(Xn−Yn)^2]
- The idea of distance measure is quite intuitive. Essentially, the observations which are closer or more similar to each other would have a low Euclidean distance and the observations 
  which are farther or less similar to each other would have a higher Euclidean distance.
- K Means Algorithm: Here we first randomly choose k number of centroids (initialization) then calulate the euclidean distance and assign to the cluster which is nearest. now Calculate 
  the centroid of the points assigned to a particular cluster in the previous step and the process is repeated untill the centroids wont change.
- In other words: Assign each observation Xi to the closest cluster centroid μk 
  		  Update each centroid to the mean of the points assigned to it.
		  The 2 steps of Assignment and Optimisation continue iteratively till the clusters stop updating.
- The cost function for the K-Means algorithm is given as: 
	 J=∑i=(1 to n)||Xi−μk(i)||^2=∑k=(1 to k)∑iϵCk||Xi−μk||^2 	[page- Module 7- KMeans Algorithm]
- In the assignment step, we assign every data point to K clusters. The algorithm goes through each of the data points and depending on which cluster is closer, in our case, whether 
  the green cluster centroid or the blue cluster centroid; It assigns the data points to one of the 2 cluster centroids.
- The equation for the assignment step is as follows:	Zi=argmin||Xi−μk||^2
- Now having assigned each data point to a cluster, now we need to recompute the cluster centroids.
- In the optimisation step, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.
- The equation for optimisation is as follows:	μk=(1/nk) ∑(i:zi=k)Xi
- The process of assignment and optimisation is repeated until there is no change in the clusters or possibly until the algorithm converges.
- KMeans++ Algorithm : This is used to initialize the k centroids to make sure that they are max dist from each other durinng initialization process instead of choosing the k centroids 
  randomly.
- First choose any one data point as 1st centroid. Now choose the second centroid on one of the other data point which is at farthest distance from the first centroid.
  Now assign all the data points to the respective cluster (nearest euclidean distance). Now choose the third centroid on the data point which has maximum distance from its corresponding 
  cluster centroid.
- Thus, the major practical considerations involved in K-Means clustering are:
	The number of clusters that you want to divide your data points into, i.e. the value of K has to be pre-determined.
	The choice of the initial cluster centres can have an impact on the final cluster formation.
	The clustering process is very sensitive to the presence of outliers in the data.
	Since the distance metric used in the clustering process is the Euclidean distance, you need to bring all your attributes on the same scale. This can be achieved through 
        standardisation.
	The K-Means algorithm does not work with categorical data.
	The process may not converge in the given number of iterations. You should always check for convergence.
- Silhouette coefficient is a measure of how similar a data point is to its own cluster (cohesion) compared to other clusters (separation).
- So to compute silhouette metric, we need to compute two measures i.e. a(i) and b(i) where, a(i) is the average distance from own cluster(Cohesion).(must be as small as possible)
											     b(i) is the average distance from the nearest neighbour cluster(Separation). (must be as 
                                                                                             large as possible)
- We combine cohesion and separation to compute the silhouette metric. S(i)=[b(i)-a(i)]/[max(a(i),b(i))]. compute this for all the data points and finally take its average which will be 
  the sillotte score for that perticular value of k.
- The Silhoutte score = 1 is the best.
- Now repeat this for different values of k and plot k v/s silhotte score. Finally choose the k which corresponds to max Silhouette score.
- To check Cluster tendency, we use Hopkins test. Hopkins test examines whether data points differ significantly from uniformly distributed data in the multidimensional space. 
  (Hopkins statestics=1 is the best ie. Good for clustering)
- On multiple iterations of Hopkins Statistic, you would be getting multiple values since the algorithm uses some randomisation in the initialisation part of the code. 
  Therefore it is advised to run it a couple of times before confirming whether the data is suitable for clustering or not.
- K-means algorithm is non-deterministic. This means that the final outcome of clustering can be different each time the algorithm is run even on the same data set. This is because, 
  as you saw, the final cluster that you get can vary by the choice of the initial cluster centres.
- the outliers have an impact on the clusters and thus outlier-infested data may not give you the most optimal clusters. 
- Similarly, since the most common measure of the distance is the Euclidean distance, you would need to bring all the attributes into the same scale using standardisation.
- We cannot use categorical data for the K-Means algorithm. There are other customised algorithms for such categorical data.
- The only ambiguous point you may notice here is that you need to decide the number of required clusters beforehand and in fact run the algorithm multiple times with a different 
  number K before you can figure out the most optimal number of clusters.
- This is also what happens in the industry practices that we run the algorithm multiple times with different values of K and then pick the clusters which make the most business sense.
- Heirarchial Clustering:
- Hierarchical Clustering. Here, instead of pre-defining the number of clusters, you first have to visually describe the similarity or dissimilarity between the different data points 
  and then decide the appropriate number of clusters on the basis of these similarities or dissimilarities.
- merging=linkage(fdf,method='complete')
  dendrogram(merging)
  plt.show()
- labels=cut_tree(merging,n_clusters=3).reshape(-1,)
- The minimum of all the pairwise distances between the data points as the representative of the distance between 2 clusters. This measure of the distance is called single linkage.
- Different types of linkages.
	Single Linkage: Here, the distance between 2 clusters is defined as the shortest distance between points in the two clusters
	Complete Linkage: Here, the distance between 2 clusters is defined as the maximum distance between any 2 points in the clusters
	Average Linkage: Here, the distance between 2 clusters is defined as the average distance between every point of one cluster to every other point of the other cluster.
- One of the major considerations in using the K-means algorithm is deciding the value of K beforehand. The hierarchical clustering algorithm does not have this restriction.
- The output of the hierarchical clustering algorithm is quite different from the K-mean algorithm as well. It results in an inverted tree-shaped structure, called the dendrogram.
- Given a set of N items to be clustered, the steps in hierarchical clustering are:
	Calculate the NxN distance (similarity) matrix, which calculates the distance of each data point from the other
	Each item is first assigned to its own cluster, i.e. N clusters are formed
	The clusters which are closest to each other are merged to form a single cluster
	The same step of computing the distance and merging the closest clusters is repeated till all the points become part of a single cluster
- Thus, what you have at the end is the dendrogram, which shows you which data points group together in which cluster at what distance.
- The result of the cluster analysis is shown by a dendrogram, which starts with all the data points as separate cluster and indicates at what level of dissimilarity any two clusters 
  were joined.
- The y-axis of the dendrogram is some measure of the dissimilarity or distance at which clusters join.
- Hierarchical clustering can proceed in 2 ways — agglomerative and divisive. If you start with n distinct clusters and iteratively reach to a point where you have only 1 cluster in the 
  end, it is called agglomerative clustering. On the other hand, if you start with 1 big cluster and subsequently keep on partitioning this cluster to reach n clusters, each containing 1 
  element, it is called divisive clustering.
- In agglomeritive Initially, n clusters are made and in each iteration, the number of clusters gets reduced by 1. So the number of iterations required is n-1 to get one big cluster. 
  Here n = number of points
- K means is used when we have huge data and Heirarchial is used when the data is of small size. This actually depends on the hardware at hand. If our hardware is good enough then we 
  can do heirarchial on huge data as well.
- K means is a Non-linear method. Heirarchial is Linear(since the datapoint does not switch to other cluster)or(because one leaf once connected by a branch cannot fall into the other 
  branch, It can only added-up, and there is no way the same element can go to the other branch/segment.)
- Computationally every time when the heirarchial process runs it consumes a humangus amount of ram bcoz every time it processes on similar size of data and it needs to hold them in 
  the memory, Hence if v r doing it in our system which has limited ram size it will stop for huge data. Still it can be done if the hardware is strong enough (cloud). So size is not 
  the constraint, It is the technology at our hand.
- K means will not consume much ram (It is computationally lighter) becoz it does not build clusters on top of each other.(everything is in one single plane)
- Industrial Hack: Here first we build heirarchial cluster and store the results of k=3 to 15 (since number of clusters less than 3 and greater than 15 is not ideal for bussiness 
  judgements) and then compare how these results/clusters and heterogenious to each other and which best suits the business problem at hand. finally after deciding the number of clusters, 
  run the heirarchial algorithm to find the centrroids and finally using this K value and the centriods we build the K-means model and obtaine the clusters.(This will help us from 
  randomly choosing centroids for K-means and also it solves the problem of finding the best value of K).
- Hierarchical clustering generally produces better clusters, but is more computationally intensive.
- we can use the dendrogram to make meaningful clusters.It is a great tool. You can look at what stage an element is joining a cluster and hence see how similar or dissimilar it is to 
  the rest of the cluster. If it joins at the higher height, it is quite different from the rest of the group. 
  You can also see which elements are joining which cluster at what stage and can thus use business understanding to cut the dendrogram more accurately.
- Average and Complete linkage methods give a well-separated dendrogram, whereas single linkage gives us dendrograms which are not very well separated. We generally want well separated 
  clusters.

******************************************************************************************************************************************************************************************
Principle Component Analysis:
- PCA is an unsupervised technique
- Principal component analysis (PCA) is one of the most commonly used dimensionality reduction techniques in the industry. By converting large data sets into smaller ones containing 
  fewer variables, it helps in improving model performance, visualising complex data sets, and in many more areas. 
- Dimensionality reduction is achieved without the loss of information.
- a couple of situations where having a lot of features posed problems for us are as follows:
	The predictive model setup: Having a lot of correlated features lead to the multicollinearity problem. Iteratively removing features is time-consuming and also leads to some 
        			    information loss.
	Data visualisation: It is not possible to visualise more than two variables at the same time using any 2-D plot. Therefore, finding relationships between the observations 
                            in a data set having several variables through visualisation is quite difficult. 
- Now, PCA helps in solving both the problems mentioned above
- Fundamentally, PCA is a dimensionality reduction technique, i.e., it approximates the original data set to a smaller one containing fewer dimensions
PCA is a dimentiaonality reduction technique which Helps:
- For data visualisation and EDA
- For creating uncorrelated features that can be input to a prediction model:  With a smaller number of uncorrelated features, the modelling process is faster and more stable as well.
- Finding latent themes in the data: If you have a data set containing the ratings given to different movies by Netflix users, PCA would be able to find latent themes like genre and, 
  consequently, the ratings that users give to a particular genre.
- Noise reduction

- what PCA does is that it converts the data by creating new features from old ones, where it becomes easier to decide which features to consider and which not to. 
- PCA is a statistical procedure to convert observations of possibly correlated variables to ‘principal components’ such that:
	They are uncorrelated with each other.
	They are linear combinations of the original variables.
	They help in capturing maximum information in the data set.
- vectors in any dimensional space or matrix can be represented as a linear combination of basis vectors. 
- This scaling and adding the vectors up to obtain a new vector is also known as a linear combination. 
- The basic definition of basis vectors is that they're a certain set of vectors whose linear combination is able to explain any other vector in that space. 
- change of basis led to dimensionality reduction.
- Mainly when we're moving between multiple basis vectors, it's important to know that the point's position in space doesn't change. The point's representation might be different 
  in different basis vectors but it would be representing the same point.
- Change in basis is the fundamental concept behind PCA. To summarise,
	PCA finds new basis vectors for us. These new basis vectors are also known as Principal Components.
	We represent the data using these new Principal Components by performing the change of basis calculations.
	After doing the change of basis, we can perform dimensionality reduction. In fact, PCA finds new basis vectors in such a way that it becomes easier for us to discard a few of 
	the features.
- M=(inv(B2)*B1). 
- v2=M*v1
- M-Transformation matrix(which transforms from one basis to another), v1-vector in current basis B1, v2- vector in transformed basis B2.
- Refer section: Fundamentals of PCA 1 - PCA and change of basis.
- Necessity for doing PCA in a couple of situations like 
	A predictive model setup where there are a lot of features to eliminate
	A dataset where you needed to perform EDA and Data Visualisation.
- You understood how PCA not only helps in resolving the above two issues but has applications in several other areas like noise reduction, finding latent Themes and so on. 
- It is a statistical procedure that finds principal components or directions that are:
	Linear combination of the original variables
	Are uncorrelated
	Capture Maximum information in the dataset.
- Standard Basis is [(1,0),(0,1)].
-From there, you learnt how to change from one basis space to another using matrices. Here's a list of rules to help you revise the same.
1.) If you're moving from a basis space B to the standard basis, then the change of basis matrix M is the same as the basis vectors of B written as its column vectors. 
    Therefore, if there is a vector v  represented in B and you want to find its representation in the standard basis, then you'd have to perform M*v
2.) If you want to go the other way around, where you have v represented in the standard basis and want to find its representation in B  you multiply it by its inverse  - inv(M)*v
3.) Finally, if you want to find the change of basis matrix M where you move from two non-standard basis vectors - say from B1 to B2 then you can get that by calculating this value - 
    inv(B2)*B1
- Note that in all the above cases, the basis vectors should be represented in the same units.
- (inverse the basis matirx (B) where we want to go)
- PCA gauges the importance of a column by another metric called ‘variance’ or how varied a column’s values are.
- measure the importance of a column by checking its variance values. If a column has more variance, then this column will contain more information.
- When the farthest points of the data are projected on the axis, the length of the projection becomes proportional to the variance
- PCA changes the basis vectors in such a way that the new basis vectors capture the maximum variance or information.
- Basically, the steps of PCA for finding the principal components can be summarised as follows.
	First, it finds the basis vector which is along the best- fit line that maximises the variance. This is our first principal component or PC1.
	The second principal component is perpendicular to the first principal component and contains the next highest amount of variance in the dataset.
	This process continues iteratively, i.e. each new principal component is perpendicular to all the previous principal components and should 
	explain the next highest amount of variance.
	If the dataset contains n independent features, then PCA will create n Principal components.
- Also, once the Principal Components are found out, PCA assigns a %age variance to each PC. Essentially it's the fraction of the total variance of the dataset explained by a 
  particular PC. This helps in understanding which Principal Component is more important than the other and by how much. 
- The steps  of PCA as summarised in the above video are as follows:
	Find n new features - Choose a different set of n basis vectors (non-standard). These basis vectors are essentially the directions of maximum variance and are 
	called Principal Components
	Express the original dataset using these new features
	Transform the dataset from the original basis to this PCA basis.
	Perform dimensionality reduction - Choose only a certain k (where k < n) number of the PCs to represent the data.  
	Remove those PCs which have fewer variance (explain less information) than others.
- PCA's role in the ML pipeline almost solely exists as a dimensionality reduction tool. Basically, you choose a fixed number of PCs that explained a certain threshold of 
  variance that you have chosen and then uses only that many columns to represent the original dataset. This modified dataset is then passed on to the ML pipeline for further 
  prediction algorithms to take place. PCA helps us in improving the model performance significantly and helps us in visualising higher-dimensional datasets as well.
- These basis vectors or directions that capture the maximum variance are essentially the Principal Components for the dataset.
- we should scale the dataset before applying PCA.
- 1. After basic data cleaning procedures, standardise your data
- 2. Once standardisation has been done, you can go ahead and perform PCA on the dataset. For doing this you import the necessary libraries from sklearn.decomposition.
- from sklearn.decomposition import PCA
- 3. Instantiate the PCA function and set the random state to some specific number so that you get the same result every time you execute that code.
- pca = PCA(random_state=42)
- 4.  Perform PCA on the dataset by using the pca.fit function. 
- pca.fit(x)
- 5. The Principal Components can be accessed using the following code:
- pca.components_
- Executing the above code will give the list of Principal components of the original dataset. They'll be of the same number as the original variables in your dataset. 
- Principle components are eigen vectors
- pca.explained_variance_ratio_ will tell us in percentage the variance explained by each PCs in decresing order
- The plot of cummulative sum of variance v/s the PCs is called the Scree Plot. This will help us to easily see how many PCs is sufficient to capture required variance.
- Here's a summary of the important steps that you performed :
1. First, you came to know how much variance is being explained by each Principal Component using the following code:
	pca.explained_variance_ratio_
- 	array([0.72770452, 0.23030523, 0.03683832, 0.00515193])
- So as you can see, the first PC, i.e. Principal Component 1([0.52  -0.26  0.58   0.56]) explains the maximum information in the dataset followed by PC2 at 23% and PC3 at 3.6%. 
  In general, when you perform PCA, all the Principal Components are formed in decreasing order of the information that they explain. Therefore, the first principal component will 
  always explain the highest variance, followed by the second principal component and so on. This order helps us in our dimensionality reduction exercise, as now we know which directions 
  are more important than the others. 
- Now, in our dataset, we only had 4 columns and equivalently 4 PCs. Therefore it was easy to visualise the amount of variance explained by them using a simple bar plot and then we're 
  able to make a call as to how much variance to keep in the data. For example, using the table above, you only need 2 principal components or 2 directions (PC1 and PC2) to explain more
  than 95% of the variation in the data.
- But what happens when there are hundreds of columns? Using the above process would be cumbersome since you'd need to look at all the PCs and keep adding their variances up to find the
  total variance captured.
2. Using a Scree-Plot:
- An elegant solution here would be to simply add a plot of "Cumulative variance explained chart". Here against each number of components, we have the total variance explained by all the
  components till then.
- So for example, cumulative variance explained by the top 2 principal components is the sum of their individual variances, given by 72.8 +23 =95.8 %. 
  Similarly, you can continue this for 3 and 4 components.
- If you plot the number of components on the X-axis and the total variance explained on the Y-axis, the resultant plot is also known as a Scree-Plot.
- Now, this is a better representation of variance and the number of components needed to explain that much variance. 
1.) Choosing the required number of components
	From the scree plot that you saw previously, you decided to keep ~95% of the information in the data that we have and for that, you need only 2 components. 
	Hence you instantiate a new PCA function with the number of components as 2. This function will perform the dimensionality reduction on our dataset and reduce 
	the number of columns from 4 to 2.
	pc2 = PCA(n_components=2, random_state=42)
2.) Perform Dimensionality Reduction on our dataset.
	Now you simply transform the original dataset to the new one where the columns are given by the Principal Components. Here you've finally performed the dimensionality 
	reduction on the dataset by reducing the number of columns from 4 to 2 and still retain 95% of the information. The code that you used to perform the same step is as follows:
	newdata = pc2.fit_transform(x)

3) Data Visualisation using the PCs
	Now that you have got the data in 2 dimensions, it is easier for you to visualise the same using a scatterplot or some other chart. By plotting the observations that we have and 
	dividing them on the basis of the species that they belong to we got the following chart:

- As you can see, you clearly see that all the species are well segregated from each other and there is little overlap between them. This is quite good as such insight was not possible 
  with higher dimensions as you won't be able to plot them on a 2-D surface. So, therefore, applying  PCA on our data is quite beneficial for observing the relationship between the 
  data points quite elegantly.

- Important Note: When you perform PCA on datasets generally, you may need more than 2 components to explain an adequate amount of variance in the data. In those cases, if you want to 
  visualise the relationship between the observations, choose the top 2 Principal Components as your X and Y axes to plot a scatterplot or any such plot to do the same. 
  Since PC1 and PC2 explain the most variance in the dataset, you'll be getting a good representation of the data when you visualise your dataset on those 2 columns.
- In the previous segments, you saw how to perform dimensionality reduction using PCA and then immediately were introduced to one of its key applications which are for data visualisation. 
  However, the most common application of PCA is to improve your model's performance. So in real life, you use PCA in conjunction with any other model like Linear Regression, 
  Logistic Regression, Clustering amongst others in order to make the process more efficient. In the following demonstration, you'll be looking at both the scenarios - 
  performing model building without PCA and then with PCA to appreciate how much faster it is to get similar or better results in the latter case.
- For this demonstration, our main model will be a logistic regression setup. As mentioned above, first we'll be performing Logistic Regression directly without any PCA. For this demo, 
  we'll be using the Telecom Churn dataset that you have worked earlier with.
- Model Building without PCA:
- You saw the process of building a churn prediction model using logistic regression. Some important problems with this process that Rahim pointed out are:
	Multicollinearity among a large number of variables, which is not totally avoided even after reducing variables using RFE (or a similar technique)
	Need to use a lengthy iterative procedure, i.e. identifying collinear variables, using variable selection techniques, dropping insignificant ones etc.
	A potential loss of information due to dropping variables
	Model instability due to multicollinearity
- If you remember the first session, we discussed all these points as potential issues that plague our model building activity. Now let's go ahead and perform PCA on the dataset and 
  then apply Logistic Regression and see if we get any better results.
- Model Building with PCA:
- In the second part, first, we'll reduce the dimensions that we have using PCA and then create a logistic regression model on it.
- As you could see, with PCA, you could achieve the same results with just a couple of lines of code. It will be helpful to note that the baseline PCA model has performed at par with the 
  best Logistic Regression model built after the feature elimination and other steps.
- PCA helped us solve the problem of multicollinearity (and thus model instability), loss of information due to the dropping of variables, and we don't need to use iterative feature 
  selection procedures. Also,  our model becomes much faster because it has to run on a smaller dataset. And even then, our ROC score, which is a key model performance metric is similar 
  to what we achieved previously.
- To sum it up, if you're doing any sort of modelling activity on a large dataset containing lots of variables, it is a good practice to perform PCA on that dataset first, reduce the 
  dimensionality and then go ahead and create the model that you wanted to make in the first place. You are advised to perform PCA on the datasets that you worked on in Linear Regression 
  and Clustering as well, to see how it makes our job easier.
- Till now, you've been looking at the scree-plot to choose the number of components that explain a certain amount of variance before going for the dimensionality reduction using PCA.
  Now, there is a nice functionality which makes this process even more unsupervised. All you need to do is select the amount of variance that you want your final dataset to capture and 
  PCA does the rest for you. Let's take a look at the following demonstration to see how we can do the same.
- As you saw above, all you needed to do was select a particular amount of variance that you want to be explained by the Principal Components of the transformed dataset. 
  PCA automatically chooses the appropriate number of components on its own and proceeds with the transformation. This again saves us a lot of time!
- Here's a summary of your learnings in the last two segments:
	You understood the importance of PCA in model building. Essentially before you build any model, you perform PCA to reduce its dimensionality.
	This results in a smaller dataset with uncorrelated features - thereby leading to faster execution and a much more stable model.
	You observed that performing PCA and then doing the actual model greatly improves its efficiency and also does that without any iterative procedures.
- Those were some important points to remember while using PCA. To summarise:
	Most software packages use SVD to compute the principal components and assume that the data is scaled and centred, so it is important to do standardisation/normalisation.
	PCA is a linear transformation method and works well in tandem with linear models such as linear regression, logistic regression, etc., though it can be used for computational 
	efficiency with non-linear models as well.
	It should not be used forcefully to reduce dimensionality (when the features are not correlated).
- You learnt some important shortcomings of PCA:
	PCA is limited to linearity, though we can use non-linear techniques such as t-SNE as well (you can read more about t-SNE in the optional reading material below).
	PCA needs the components to be perpendicular, though in some cases, that may not be the best solution. The alternative technique is to use Independent Components Analysis. 
	PCA assumes that columns with low variance are not useful, which might not be true in prediction setups (especially classification problem with a high class imbalance).
- Here's a list of useful functions that use after importing the PCA function from sklearn libraries.
	pca.fit() - Perform PCA on the dataset.
	pca.components_ -  Explains the principal components in the data
	pca.explained_variance_ratio_ - Explains the variance explained by each component
	pca.fit(n_components = k) - Perform PCA and choose only k components
	pca.fit_transform  - Transform the data from original basis to PC basis.
	pca(var) -  Here 'var' is a number between 0-1. Perform PCA on the dataset and choose the number of components automatically such that the variance explained is (100*var)%.

********************************************************************************************************************************************************************************************
Support Vector Machines:
- SVMs have been extensively used for solving complex classification problems such as image recognition, voice detection etc.
- SVMs are mostly used for classification tasks, but they can also be used for regression.
- Besides their ability to solve complex machine learning problems, they have numerous other advantages over other classification problems, such as the ability to deal with 
  computationally heavy data sets, classifying nonlinearly separable data, etc.
- SVMs belong to the class of linear machine learning models (logistic regression is also a linear model).
- A linear model uses a linear function (i.e. of the form y = ax +b) to model the relationship between the input x and output y. 
- SVMs are linear models that require numeric attributes. In case the attributes are non-numeric, you need to convert them to a numeric form in the data preparation stage.
- Hyperplane is a boundary which 'separates' the data set into its classes (in this case, separates spam emails from the ham ones). It could be lines, 2D planes, or even n-dimensional
  planes that are beyond our imagination.
- A line that is used to classify one class from another is also called a hyperplane. In fact, it is the model you're trying to build.
-The standard equation of a line is given by ax+by+c = 0. You could generalise it as W0+W1x1+W2x2=0, where x1 and x2 are the features — such as 'word_freq_technology' and
 'word_freq_money' — and W1 and W2 are the coefficients. For any line with W coefficients, substituting the value of features x1 and x2 in the equation of the line determined by 
  its W coefficients, will return a value. 
- A positive value (blue points in the plot above) would mean that the set of values of the features is in one class; however, a negative 
  value (red points in the plot above) would imply it belongs to the other class. A value of zero would imply that the point lies on the line (hyperplane) because any point on the 
  line will satisfy the equation: W0+W1x1+W2x2=0. 
- The classification rule to classify a general point (p, q) is given by: (W1.p−q+W0).Y>=0 
- Dimension of a hyperplane in a 3D space will be [number of features - 1]. If you look at the 3D plot as well, you will see that the data can be easily separated by a plane.
- In general, if you derive the d-dimensional hyperplane from d attributes, you can write the expression as follows: ∑(i=1 to d)[(Wi.Xi)+W0=0].
- The model denoted by the expression given above is called a linear discriminator. 
- The best line is the one that maintains the largest possible equal distance from the nearest points of both the classes. It is also referred to as a maximal margin classifier. 
  You can think of the margin as a 'band' that the hyperplane has on both its sides.
- There can be several lines (hyperplanes) possible in the same data set, with different values of margins. Among these, 
  the line with the maximum margin would be considered the best fit line for the given data.
- Shortest(perpendicular) distance between hyperplane(W0+W1x1+W2x2=0) and point(p,q) is d= (W0+W1p+W2q)/sqrt(W1^2+W2^2).
- The mathematical formulation requires two major constraints that need to be taken into account while maximising the margin. They are 
  The standardisation of coefficients such that the summation of the square of the coefficients of all the attributes is equal to 1. For example, 
  if you have 20 attributes, then the summation of square of the coefficients should be  ∑(i=0 to 20)[(Wi^2)] =1. Along with the first constraint, the maximal margin hyperplane 
  should also follow the constraint given below:(li∗(Wi.Yi))⩾M where, 	li =  represents the label of the ith observation (such as spam(+1) and ham(-1));
									Wi = represents the vector of the coefficients (or weights) of each attribute (for example, if you have 3 attributes,
										 W = [w0, w1, w2, w3]). 
									Yi  = represents the vector of the attribute values for the ith row, e.g. Y = [1,y1, y2, y3] for 3 attributes. 
- Let's understand these contraints through an example, since you already know how to write an equation for a hyperplane in 2D. Now say that the hyperplane is represented as 
  W0+W1X1+W2X2=0. So what you need to do is apply both the constraints in this equation such that you get a maximal margin hyperplane equation that maintains an equal distance 
  from both the labels. The first constraint should be valid if W20+W21+W22=1.
- For the second constraint, the hyperplane should be a safe distance of more than or equal to the margin 'M' from both the labels. If you look at figure 2(A), all the red dots are 
  below the hyperplane that is labelled (-L). Here, the multiplication of (-L), with the dot product of Y(p,q) and the equation W0+W1X1+W2X2, will give you a positive value 
  that is greater than or equal to  'M'.
- Similarly, if you look at figure 2(B), all the blue dots are above the hyperplane that is labelled (+L). Here, the multiplication of (+L), with the dot product of Y(p,q) 
  and the normalized vector, W0+W1X1+W2X2 , will also give you a positive value that is greater than or equal to 'M'.
- A margin is calculated by considering only the points closest to the hyperplane.
- The Maximal Margin Classifier is better than the others because it maintains an equal distance from both the classes; this performs better on the test set.
- The better one is a 'safe' distance away from both the classes and thus, will minimise the chances of incorrect identification.
- The margin should be selected in such a way that it has the maximum distance from both the classes. For example, if you want to categorise spam and ham by a plane, 
  then the plane should be drawn in such a way that it is equally set apart from both the classes, i.e. 'spam' and 'ham'.
- The Maximal Margin Classifier is the best hyperplane that maintains an equal distance from the closest points in both the classes. This distance is called the margin.
-  Consider a data set with two independent variables, say X1 and X2, and one dependent binary variable, Y, with two class labels denoted by +1 and -1. The data set is plotted, 
  along with a separating hyperplane (h).
- Let's say that the class label of the i-th data point is denoted by li (it can take the values -1 and 1). The weights are denoted by w0,w1,w2 and the vector of the i-th 
  data point is denoted by Yi. The following two constraints determine the equation of the maximal margin classifier: 	∑(i=0 to n)[(Wi^2)]=1 
															(liX(Wi.Yi))⩾M 
  The first constraint simply normalises the coefficients (weights) of the hyperplane so that their sum of their squares is one. For e.g. the line x+y+1=0 originally has weights 
  (w0,w1,w2)=(1,1,1) which in the normalised form become (w0,w1,w2)=(1√3,1√3,1√3). In other words, the equation of the line x+y+1=0 can be written as x/√3+y/√3+1/√3=0 in the 
  normalised form.
- The second constraint ensures that all data points are at least M distance away from the hyperplane, i.e. the margin is M. The absolute value of the dot product Wi.Yi is the 
  distance of the point i from the hyperplane. The dot product is positive if the point i lies below the hyperplane (blue), else negative.
- Maximal Margin Classifier separates the two classes perfectly.
- But the maximal margin classifier has some serious limitations. For example, it is possible only on data sets which are perfectly linearly separable, which occur rarely in real life. 
- the classifier which solves these problems - the Soft Margin Classifier, which allows certain points to be deliberately misclassified. 
- although the Maximal Margin Classifier (i.e hyperplane) perfectly separates the two classes, it has a rather limited applicability: it cannot classify data points if 
  they are partially intermingled
- support vectors are the points that lie close to the hyperplane. In fact, they are the only points that are used in constructing the hyperplane.
- SVCs(Support Vector Classifiers) are relatively immune to outliers because SVCs are formulated from the support vector points. It implies that the SVC (i.e hyperplane) 
  will not be changed if we do not change the support vectors.
- The hyperplane that allows certain points to be deliberately misclassified is also called the Support Vector Classifier. 
  The support vector classifier works well when the data is partially intermingled (i.e. most of the data can be classified correctly with some misclassifications).
  Similar to the Maximal Margin Classifier, the Support Vector Classifier also maximises the margin; but it will also allow some points to be misclassified
- The soft margin is used in constructing the Soft Margin Classifier( Support Vector Classifier) which allows some points to be misclassified, 
  whereas the hard margin ensures no points are misclassified.
- Slack variable(ϵ). A slack variable is used to control misclassifications. It tells you where an observation is located relative to the margin and hyperplane. For points which are at 
  a distance of more than M, i.e. at a safe distance from the hyperplane, the value of the slack variable is 0. 
- On the other hand, if a data point is correctly classified but falls inside the margin (or violates the margin), then the value of its slack ϵ is between 0 and 1. In the plot given below,
  the blue and the red points, which fall inside the margin, are examples of such points.
- Finally, if a data point is incorrectly classified (i.e. it violates the hyperplane), the value of epsilon (ϵ) > 1. The incorrectly classified blue and red points are such examples. 
- So you can see that
	Each data point has a slack value associated to it, according to where the point is located.
	The value of slack lies between 0 and +infinity.
	Lower values of slack are better than higher values (slack = 0 implies a correct classification, but slack > 1 implies an incorrect classification, whereas slack 
	within 0 and 1 classifies correctly but violates the margin).
- Once you understand the notion of the slack variable, you can easily compare any two Support Vector Classifiers. You can measure the summation of all the epsilons(ϵ) of 
  both the hyperplanes and choose the best one that gives you the least sum of epsilons(ϵ). The summation of all the epsilons of each data point is denoted by cost or 'C', i.e.
  ∑ϵi≤C.
- When C is large, the slack variables can be large, i.e. you allow a larger number of data points to be misclassified or to violate the margin. So you get a hyperplane where 
  the margin is wide and misclassifications are allowed. In this case, the model is flexible, more generalisable, and less likely to overfit. In other words, it has a high bias. 
- On the other hand, when C is small, you force the individual slack variables to be small, i.e. you do not allow many data points to fall on the wrong side of the margin or 
  the hyperplane. So the margin is narrow and there are few misclassifications. In this case, the model is less flexible, less generalisable, and more likely to overfit. 
  In other words, it has a high variance. 
- When C is large, the slack variables can be large, i.e. the model allows a larger number of data points to be misclassified or violate the margin. In this case, 
  the model is flexible, more generalisable, and less likely to overfit. In other words, it has a high bias. As you learnt in the model selection lectures, if you apply 
  this model to unseen data, it can result in less variance.
- If the value of C is very low, then there will be no misclassifications; this may overfit the training data, and the model becomes less generalisable. 
  Similarly, when the value of C is very high, then many points will be misclassified; this results in a bad model. So it is better to set the value of C to moderate.
- Adding more training points and increasing value of C will reduce model overfitting problem. Because if you train the model on a large number of data, it will try to 
  learn from the data rather fitting to the each data points. and if you increase the value of C, ultimately you are allowing your model to misclassify some data points. 
  And, hence, the model will not be overfitted in these two cases.
- Thus, the dot product W.Yi is simply the value of the expression obtained by putting the ith data point in the hyperplane equation, i.e. W.Yi=w0+w1y1+w2y2+w3y3. 
- Thus, W.Yi is lesser than, equal to or greater than 0, depending on the location of the ith data point with respect to the hyperplane. Also, note that the value of W.Yi 
  gives you the distance of the ith data point from the hyperplane.
- M represents the margin, i.e. the distance of the closest data point from the hyperplane.
- If you impose the condition (liX(W.Yi)>=M) on the model, then you are implying that you want each point to be at least a distance M away from the hyperplane. But unfortunately, 
  few real datasets will be so easily, perfectly separable. Thus, to relax the constraint, you include a ‘slack variable’ εi for each data point i.
- Thus, you modify the formulation to (liX(W.Yi)>=M(1−εi), Where the slack variable (εi) takes a value between 0 to infinity
- Depending on the value of εi, the ith data point can now take any position - it can fall on the correct side of the margin (and a safe distance away), or inside the margin 
  (but still correctly classified), or even stray on the wrong side of the hyperplane itself.
- If C is large, then the slack variables ​εi​ can take higher values. And you know that when slack(​εi​) > 1, the point is misclassified, between 0 and 1, it falls inside the margin, 
  and when slack(​εi​) = 0, it is correctly classified. Thus, for higher C, more points will be allowed to get misclassified or fall inside the margin (compared to a lower C). 
  On the other hand, a lower C implies that each slack(​εi​) will have to take a lower value, and thus not be allowed to stray on the other side of the margin or the hyperplane. 
  This is a more strict condition.In other words, a lower C does not give the model freedom to misclassify even a few points, and thus the model tries to overfit the data.
-  C is the sum of all the values of slack variables. The parameter cost of misclassification (C) represents the cost of violations to the margin and the hyperplane. 
  Thus, the parameter C is called the tuning parameter — one of the hyperparameters in SVM.
- When C is large, the slack variables can be large, i.e. you allow a larger number of data points to be misclassified or to violate the margin. So you get a hyperplane where 
  the margin is wide and misclassifications are allowed. And hence, it will work well on unseen data.
- Misclassification can be controlled by the value of cost or C. If C is large, the slack variables (epsilons( ϵ)) can be large, i.e. you allow a larger number of data points 
  to be misclassified or violate the margin; and if C is small, you force the individual slack variables to be small, i.e. you do not allow many data points to fall on the wrong side
  of the margin or the hyperplane.
- So this is how the cost 'C' influences your decision of a best-fit hyperplane.
- rescaling is an important step since some features may range from a extremely small range of numbers (say fractions) and others may be orders of magnitude higher (say 10k-100k). 
  This may cause some features (in SVMs, they are 'dimensions') whose values are higher to dominate over others.
- The professor pointed out the importance of reading documentation before using libraries. In fact, it is quite important to do that before implementing SVM in python, 
  since the meaning of the hyperparameter C in sklearn is quite different. 
 Important Note: The hyperparameter C has a different meaning in the python SVC() implementation
- Please note that the parameter C that you used in the SVM formulation (in the theory lectures) and the C in the SVC() function are the inverse of each other.
- Previously, we discussed how C is the sum of all the slack variables, and thus, a large value of C implies that you are willing to accommodate a higher number of misclassifications 
  (and thus, the model is more generalisable since it does not have to worry about classifying all the points correctly). According to this definition, a large C ensures that the model 
  will not overfit. 
- In other words, a high value of the parameter C, as discussed earlier, regularizes the SVM model.
- On the other hand, in the SVC() implementation of python that you will use, the hyperparameter C is analogous to the penalty imposed for misclassification, i.e. a higher C will force
  the model to classify most (training) data points correctly (and thus, overfit). In sklearn, high value of C implies a high cost of making errors or misclassifications.
- Although the value of the hyperparameter C is chosen using cross-validation, it is helpful to understand its meaning clearly and avoid confusion. 
- To summarise, the C you used in the SVM formulation and the C in the ksvm() parameter are the inverse of each other. So a high value of C will not accommodate many misclassifications, 
  while a low value will allow misclassification in the SVC() parameter. 
- optimal value of the hyperparameter(C [0.1,1,10,100,1000] and gamma) varies significantly with the choice of evaluation metric.
- Maximal Margin Classifier  will not find a separator if the classes are not linearly separable.
- C represents the 'liberty of misclassification' and In SVC(), C represents the 'penalty for misclassification'. 
- The data points on the edge of the ‘band’, around the separator that makes all the data points outside the band redundant, are called Support vectors.
- Kernels are one of the most interesting inventions in machine learning, partly because they were born through the creative imagination of mathematicians, and partly because of their 
  utility in dealing with non-linear datasets. 
- You’ll agree that it is not possible to imagine a linear hyperplane (a line in 2D) that separates the red and blue points reasonably well. Thus, you need to tweak the linear SVM model 
  and enable it to incorporate nonlinearity in some way.
- Kernels serve this purpose — they enable the linear SVM model to separate nonlinearly separable data points.
- it is important to remember that SVMs are linear models, and kernels do not change this at all.  kernels are ‘toppings’ over the linear SVM model, 
  which somehow enable the model to separate nonlinear data.
- you can transform nonlinear boundaries to linear boundaries by applying certain functions to the original attributes. The original space (X, Y) is called the original attribute space, 
  and the transformed space (X’, Y’) is called the feature space.
- The process of transforming the original attributes into a new feature space is called ‘feature transformation’.
- there is an exponential increase in the number of dimensions when you transform the attribute space to a feature space. This makes the modelling (i.e. the learning process) 
  computationally expensive. solve this problem using kernels.
- Similarly, in any given dataset with n attributes, you can think of each data point (i.e. each row) as a vector of length n. Thus, the dot product of two rows, or observations, 
  will return a scalar value. 
- Think of a kernel as a black box. The original attributes are passed into the black box, and it returns the transformed attributes 
  (in a higher dimensional feature space). The SVM algorithm is shown only the transformed, linear feature space, where it builds the linear classifier as usual.  
- However, what makes kernels special is that they don't do this transformation explicitly (which is a computationally difficult task, as we discussed), but they use a 
  mathematical hack to do this implicitly.
- Pre-reading - Inner Product 
- Here, you will need to recall the concept of the inner product (or dot product) of two vectors. Say there are two vectors A = [2, 3, 5] and B = [1, 0, 4], 
  then the dot product (or the inner product) of A and B is the element-wise multiplication of A and B, i.e. transpose(A). B = A'B = 2.1 + 3.0 + 5.4 = 22. 
-  Note that the inner product of two vectors returns a scalar number, in this case 22.
- To summarise, the key fact that makes the kernel trick possible is that to find a best fit model, the learning algorithm only needs the inner products of the observations (XTi.Xj). 
  It never uses the individual data points X1, X2 etc. in silo.
- Kernel functions use this fact to bypass the explicit transformation process from the attribute space to the feature space, and rather do it implicitly. 
  The benefit of implicit transformation is that now you do not need to:
	Manually find the mathematical transformation needed to convert a nonlinear to a linear feature space
	Perform computationally heavy transformations .
- In practice, you only need to know that kernels are functions which help you transform non-linear datasets. Given a dataset, you can try various kernels, 
  and choose the one that produces the best model. The three most popular types of kernel functions are: 
	The linear kernel: This is the same as the support vector classifier, or the hyperplane, without any transformation at all
	The polynomial kernel: It is capable of creating nonlinear, polynomial decision boundaries 
	The radial basis function (RBF) kernel: This is the most complex one, which is capable of transforming highly nonlinear feature spaces to linear ones. 
	It is even capable of creating elliptical (i.e. enclosed) decision boundaries.
- The three types of kernel functions shown in the figures below represent the typical decision boundaries they are capable of creating. Notice that the linear kernel is same as the 
  vanilla hyperplane, the polynomial kernel can produce polynomial shaped nonlinear boundaries, and the RBF kernel can produce highly nonlinear, ellipsoid shaped boundaries.
-As you have learnt, the linear kernel, also known as the hyperplane, requires only one tuning parameter, i.e. 'C' to select the best-fit linear model.
- In a non-linear kernel, such as the RBF kernel, you'll need to choose two tuning parameters: gamma and 'C'. The hyperparameter gamma controls  the amount of non-linearity in the 
  model - as gamma increases, the model becomes more non-linear, and thus model complexity increases.
- Also, now since you have two hyperparameters to optimise (C and gamma), you would need to define a range of values of 'C' and gamma. As you would have guessed, grid search 
  cross-validation is the best way to choose the best combination of these hyperparameters.
- the model tends to overfit at higher values of gamma (keeping C constant).
- increasingly complex (non-linear) models result in a higher training accuracy, though the test accuracy does not increase significantly with increasing model complexity.
- High values of gamma lead to overfitting (especially at high values of C); note that the training accuracy at gamma=0.01 and C=1000 reaches almost 99%
- The training score increases with higher gamma, though the test scores are comparable (at sufficiently high cost, i.e. C > 10)
- The least amount of overfitting (i.e. the difference between train and test accuracy) occurs at low gamma, i.e. a quite simple non-linear model
- In nonlinear kernels such as the RBF, you use the parameter gamma to control the amount of nonlinearity in the model. The higher the value of gamma, the more 
  is the nonlinearity introduced; the lower the value of gamma, the lesser is the nonlinearity. It is also denoted as sigma in some texts and packages. 
- Apart from gamma, you also have the hyperparameter C, or the cost (with all types of kernels).-
-'C', gamma, and the types of kernels, all have an effect on constructing the decision boundary.
- When gamma is high, more nonlinearity is added. When you increase the gamma from 10 to 100, the nonlinearity is further increased, and all the training points are correctly mapped
- A higher value of 'C' ( tuning parameter i.e used in SVC() in Python lab) will not allow any points to be misclassified. The SVM model will be 
  overfitting when there is no misclassification.
- A higher value of gamma will add more nonlinearity to the decision surface. The SVM model will be overfitting when no points are misclassified, 
  and the nonlinearity is highly introduced than required.
- Choosing the appropriate kernel is important for building a model of optimum complexity. If the kernel is highly nonlinear, the model is likely to overfit. On the other hand, if the 
  kernel is too simple, then it may not fit the training data well. 
- Usually, it is difficult to choose the appropriate kernel by visualising the data or using exploratory analysis. Thus, cross-validation (or hit-and-trial, if you are only choosing 
  from 2-3 types of kernels) is often a good strategy.
- In 2D and 3D, it is very useful to visualise independent variables graphically; this can help you apply best-fit transformations that classify labels (remember the spam and ham example) 
  correctly.
- But the graphical representation of independent variables is not so easy for more than three attributes. Even the manual transformation of an attribute space to a feature 
  space is a cumbersome process. To overcome this challenge, you use the kernel trick for transformation, which bypasses the process of converting an attribute space to a feature 
  space explicitly. This means that now, you don't explicitly need to convert each attribute to a feature space. Instead, you need to find a function(∅) that achieves all the manual 
  steps in one go.
- Essentially, it helps you identify the best-fit linear separable boundaries by trying out various transformations within the function in the higher dimension. 

*********************************************************************************************************************************************************************************************
 TREE MODELS:
- A collection of multiple models is called an ensemble.
- With high interpretability and an intuitive algorithm, decision trees mimic the human decision-making process and are efficient in dealing with categorical data. 
  Unlike other algorithms, such as logistic regression and support vector machines (SVMs), decision trees do not help in finding a linear relationship between the independent 
  variable and the target variable. However, they can be used to model highly non-linear data.
- You can use decision trees to explain all the factors that lead to a particular decision/prediction. And so can be used in explaining certain business decisions to entrepreneurs. 
  Decision trees form the building blocks for random forests, which are commonly used among the Kaggle community.
- Random forests are collections of multiple trees and are considered to be one of the most efficient machine learning models.
- Linear models cannot handle collinearity and non linear relationships in the data well. Now here comes the role of decison trees which leverages these properties.
- A decision tree, as the term suggests, uses a tree-like model to make predictions. It resembles an upside-down tree and uses a similar process that you do to make decisions in real life,
  i.e., by asking a series of questions to arrive at a decision.
- A decision tree splits data into multiple sets of data. Each of these sets is then further split into subsets to arrive at a decision.
- a decision tree uses a natural decision-making process, i.e., it asks a series of questions in a nested if-then-else structure. On each node, you ask a question to further split the 
  data that is held by the node. If the test passes, you move to the left; otherwise, you move to the right.
- The first and top node of a decision tree is called the root node. The arrows in a decision tree always point away from this node.
- The node that cannot be further classified or split is called the leaf node. The arrows in a decision tree always point towards this node.
- The intermediate nodes between the root and the leaf nodes are called the internal nodes.
- a decision tree is nothing but a tree asking a series of questions to arrive at a prediction. The problem at hand is to predict whether a person has heart disease or not. Based on the 
  values that various attributes such as gender, age, cholesterol, the decision trees try to make a prediction and output a flowchart-like diagram
- it is easy to interpret a decision tree, and you can almost always identify the various factors that lead to a particular decision. In fact, trees are often underestimated in their 
  ability to relate the predictor variables to their predictions. As a rule of thumb, if interpretability by layman is what you are looking for in a model, then decision trees should be 
  at the top of your list.
- So, as you saw in the video, constructing a decision tree involves the following steps:
	Recursive binary splitting/partitioning the data into smaller subsets
	Selecting the best rule from a variable/ attribute for the split
	Applying the split based on the rules obtained from the attributes
	Repeating the process for the subsets obtained
	Continuing the process until the stopping criterion is reached
	Assigning the majority class/average value as the prediction
- The decision tree building process is a top-down approach. The top-down approach refers to the process of starting from the top with the whole data and gradually splitting the data into 
  smaller subsets. 
- The reason we call the process greedy is because it does not take into account what will happen in the next two or three steps. The entire structure of the tree changes with small
  variations in the input data. This, in turn, changes the way you split and the final decisions altogether. This means that the process is not holistic in nature, as it only aims to gain 
  an immediate result that is derived after splitting the data at a particular node based on a certain rule of the attribute.
- You can see that the model that we have now is not performing well on the test set. This is because we built our model on the default parameters except for the depth and didn’t change 
  any other hyperparameters. Hyperparameter tuning can improve the performance of decision trees to a great extent. So in the upcoming sessions, we will go ahead and exploit these 
  parameters to improve the model and give better prediction results. 

- What are hyperparameters?
- Hyperparameters are simply the parameters that we pass on to the learning algorithm to control the training of the model. Hyperparameters are choices that the algorithm designer makes
  to ‘tune’ the behaviour of the learning algorithm. The choice of hyperparameters, therefore, has a lot of bearing on the final model produced by the learning algorithm.  
- So basically anything that is passed on to the algorithm before it begins its training or learning process is a hyperparameter, i.e., these are the parameters that the user provides 
  and not something that the algorithm learns on its own during the training process. Here, one of the hyperparameters you input was "max_depth" which essentially determines how many 
  levels of nodes will you have from root to leaf. This is something that the algorithm is incapable of determining on its own and has to be provided by the user. 
  Hence, it is a hyperparameter.
- Now, obviously, since hyperparameters can take many values, it is essential for us to determine the optimal values where the model will perform the best. This process of 
  optimising hyperparameters is called hyperparameter tuning. 
- There are certain cases where you cannot directly apply linear regression to solve a regression problem. Linear regression fits only one model to the entire data set; however,
  you may want to divide the data set into multiple subsets and apply decision tree algorithm in such cases to handle non-linearity.
- Let’s summarise the advantages of tree models one by one in the following order:
	Predictions made by a decision tree are easily interpretable.
	A decision tree is versatile in nature. It does not assume anything specific about the nature of the attributes in a data set. It can seamlessly handle all kinds of data such as 
	numeric, categorical, strings, Boolean, etc.
	A decision tree is scale-invariant. It does not require normalisation, as it only has to compare the values within an attribute, and it handles multicollinearity better.
	Decision trees often give us an idea of the relative importance of the explanatory attributes that are used for prediction.
	They are highly efficient and fast algorithms.
	They can identify complex relationships and work well in certain cases where you cannot fit a single linear relationship between the target and feature variables. This is where 
	regression with decision trees comes into the picture.
- In regression problems, a decision tree splits the data into multiple subsets. The difference between decision tree classification and decision tree regression is that in regression, 
  each leaf represents the average of all the values as the prediction as opposed to a class label in classification trees. For classification problems, the prediction is assigned to a 
  leaf node using majority voting but for regression, it is done by taking the average value.
- Decision tree classification is what you’ll most commonly work on. However, remember that if you get a data set where you want to perform regression, decision tree regression is also a 
  good idea.
- In decision trees, you do not have to treat missing values, outliers and multicollinearity before proceeding with model building.
- Decision trees are easy to interpret, as you can always traverse backwards and identify the various factors that lead to a particular decision. A decision tree requires you to 
  perform certain tests on attributes in order to split the data into multiple partitions.
- In classification, each data point in a leaf has a class label associated with it.
- You cannot use the linear regression model to make predictions when you need to divide the data set into multiple subsets, as each subset has an independent trend corresponding to it.
  In such cases, you can use the decision tree model to make predictions because it can split the data into multiple subsets and assign average values as the prediction to each set 
  independently.
- It is easy to infer that to predict the target variable (in this case, heart disease), there are obviously some questions/attributes that are more important for its prediction than 
  others. 
- A split that results in a homogenous subset is much more desirable than the one that results in a 50-50 distribution (in the case of two labels). In a completely homogenous set,
  all the data points belong to one particular label. Hence, you must try to generate partitions that result in such sets.
-For classification purposes, a data set is completely homogeneous if it contains only a single class label. For regression purposes, a data set is completely homogeneous if its 
 variance is as small as possible. You will understand regression trees better in the upcoming segments
- Consider a data set ‘D’ with homogeneity ‘H’ and a defined threshold value. When homogeneity exceeds the threshold value, you need to stop splitting the node and assign the 
  prediction to it. As this node does not need further splitting, it becomes the leaf node.
- Till the homogeneity ‘H’ is less than the threshold, you need to continue splitting the node. The process of splitting needs to be continued until homogeneity exceeds the threshold 
  value and the majority data points in the node are of the same class.
- This is an abstract example to give you an intuition on homogeneity and splitting. You will get better clarity in the next segment when you learn how to quantify and measure 
  homogeneity to arrive at a prediction through continuous splitting. You will learn how to use specific methods to measure homogeneity, namely the Gini index, entropy, 
  classification error (for classification), and MSE (for regression).
-  If an attribute is nominal categorical, then there are [2^(k−1)]−1 possible splits for this attribute, where k is the number of classes.
- If an attribute is ordinal categorical or continuous in nature with n different values, there are n - 1 different possible splits for it. Each value of the attribute is sorted from the 
  smallest to the largest and candidate splits based on the individual values is examined to determine the best split point which maximizes the homogeneity at a node. 
-The ultimate aim of decision tree splitting is to Increase homogeneity.
- Out of all the attributes, the attribute that results in the maximum increase in homogeneity is chosen for splitting.
- Various methods, such as the classification error, Gini index and entropy, can be used to quantify homogeneity.
- The classification error is calculated as follows: 	E=1−max(pi)
  The Gini index is calculated as follows: 		G=∑(i=1 to k)[pi(1−pi)] 
  Entropy is calculated as follows: 			D=−[∑(i=1 to k)[pi.log2(pi)]], where pi is the probability of finding a point with the label i, and k is the number of classes.(log base 2)
- Recall the graph of Entrop, Gini index, classification error v/s probability. Ref- Trre models-Algoriths for DTC- impurity measures- video 2
- 
Impurity Measures	
		Case I 	Class 0: 20	Case II	Class 0: 50	Case IIIClass 0: 80
			Class 1: 80		Class 1: 50		Class 1: 20

Classification Error	0.2			0.5			0.2
Gini Impurity		0.32			0.5			0.32
Entropy			0.72			1			0.72

- You can see that for a completely non-homogeneous data with equal class distribution, the value of Classification Error and Gini Impurity are the same i.e. 0.5 and that of Entropy is 1.
- The scaled version of the entropy in the illustration shown in the video is nothing but entropy/2. It has been used to emphasize that the Gini index is an intermediate measure between 
  entropy and the classification error.
- In practice, classification error does not perform well. So, we generally prefer using either the Gini index or entropy over it.
- Gini Index: Gini index is the degree of a randomly chosen datapoint being classified incorrectly. 
- Gini index of 0 indicates that all the data points belong to a single class. Gini index of 0.5 indicates that the data points are equally distributed among the different classes.
- Suppose you have a data set with two class labels. If the data set is completely homogeneous, i.e., all the data points belong to label 1, then the probability of finding a data point 
  corresponding to label 2 will be 0 and that of label 1 will be 1. So, p1 = 1 and p2 = 0. The Gini index, which is equal to 0, will be the lowest in such a case. Hence, the higher the 
  homogeneity, the lower the Gini index.
- Entropy: Entropy quantifies the degree of disorder in the given data, its value varies from 0 to 1. Entropy and the Gini index are similar numerically. If a data set is completely 
  homogenous, then the entropy of such a data set will be 0, i.e., there is no disorder in the data. If a data set contains an equal distribution of both the classes, then the entropy of 
  that data set will be 1, i.e., there is complete disorder in the data. Hence, like the Gini index, the higher the homogeneity, the lower the entropy. 
- The change in impurity or the purity gain is given by the difference of impurity post-split from impurity pre-split, i.e.,
  Δ Impurity = Impurity (pre-split) – Impurity (post-split)
- The post-split impurity is calculated by finding the weighted average of two child nodes. The split that results in maximum gain is chosen as the best split.
- To summarise, the information gain is calculated by: Gain=D−DA
  where D is the entropy of the parent set (data before splitting),DA is the entropy of the partitions obtained after splitting on attribute A
- Note that reduction in entropy implies information gain.
-  You always try to maximise information gain by achieving maximum homogeneity and this is possible only when the value of entropy decreases from the parent set after splitting.
- In case of a classification problem, you always try to maximise purity gain or reduce the impurity at a node after every split and this process is repeated till you reach the leaf 
  node for the final prediction. 
- Considering all the data points in a data set have the same label, what will be the Gini index? = 0
- In a given data set, 50% of the data points belong to label 1, and the other 50% belong to label 2. Calculate the Gini index. = 0.5
- When is the Gini index of a data set high? = When the homogeneity is minimum.
- When is the information gain maximum? = When the decrease in entropy, from the parent set to the partitions obtained after splitting, is maximum.
- How is entropy related to the Gini index? = The higher the entropy, the higher is the Gini index.
- Information Gain= Gini impurity befor split - Gini impurity after split.
- Thus, the split based on gender(particular attribute) gives the following insights:
	Gini impurity before split = 0.48		 (p(1-p)+(1-p)p)
	Gini impurity after split = 0.42		 this is the weighted average of gini indexs of splitted nodes (0.6(gini index of left node)+ 0.4(gini index of right node))
	Reduction in Gini impurity = 0.48 - 0.42 = 0.06  information gain/purity gain for the corresponding attribute.
- Low gini index = high homoginity (preferable)
- Let's summarise all the steps you performed.
	Calculate the Gini impurity before any split on the whole dataset.
	Consider any one of the available attributes.
	Calculate the Gini impurity after splitting on this attribute for each of the levels of the attribute. In the example above, we considered the attribute 'Sex' and then 
	calculated the Gini impurity for both males and females separately.
	Combine the Gini impurities of all the levels to get the Gini impurity of the overall attribute.
	Repeat steps 2-5 with another attribute till you have exhausted all of them.
	Compare the decrease in Gini impurity across all attributes and select the one which offers maximum reduction.
- Feature importance plays a key role in contributing towards effective prediction, decision-making and model performance. It eliminates the less important variables from a large data
  set and helps in identifying the key features that can lead to better prediction results.
- Decision trees help in quantifying the importance of each feature by calculating the reduction in the impurity for each feature at a node. The feature that results in a significant
  reduction in the impurity is the important variable, and the one that results in less impurity reduction is the less important variable.
- picking an attribute and a rule to split data into multiple partitions to increase the homogeneity of the data set
- You also learnt about the various ways in which you can measure the homogeneity of a data set, such as the Gini index, entropy and MSE.
- Now, let’s summarise your learnings so far:
	A decision tree first decides on an attribute to split on.
	To select this attribute, it measures the homogeneity of the nodes before and after the split.
	You can measure homogeneity in various ways with metrics like Gini index and entropy.
	The attribute that results in the increase of homogeneity the most is then selected for splitting.
	Then, this whole cycle is repeated until you obtain a sufficiently homogeneous data set.
- As can be seen by fully growing the tree that every leaf is a pure leaf (completely homogenous) and hence, the training error is zero.
- Hyperparameter Tuning in Decision Trees:
- The following is a summary of the disadvantages of decision trees:
	They tend to overfit the data. If allowed to grow with no check on its complexity, a decision tree will keep splitting until it has correctly classified (or rather, mugged up) 
	all the data points in the training set.
	They tend to be quite unstable, which is an implication of overfitting. A few changes in the data can considerably change a tree.
- There are two broad strategies to control overfitting in decision trees: truncation and pruning. In this video, you will learn how these two techniques help control overfitting.
- here are two ways to control overfitting in trees:
	Truncation - Stop the tree while it is still growing so that it may not end up with leaves containing very few data points. Note that truncation is also known as pre-pruning.
	Pruning - Let the tree grow to any complexity. Then, cut the branches of the tree in a bottom-up fashion, starting from the leaves. It is more common to use pruning strategies 
	to avoid overfitting in practical implementations.
- Though there are various ways to truncate or prune trees, the DecisionTreeClassifier() function in sklearn provides the following hyperparameters which you can control:
	- criterion (Gini/IG or entropy): It defines the homogeneity metric to measure the quality of a split. Sklearn supports “Gini” criteria for Gini Index & “entropy” for Information 
	  Gain. By default, it takes the value of “Gini”.
	- max_features: It defines the no. of features to consider when looking for the best split. We can input integer, float, string & None value.
	   1. If an integer is inputted then it considers that value as max features at each split.
	   2. If float value is taken then it shows the percentage of features at each split.
	   3. If “auto” or “sqrt” is taken then max_features=sqrt(n_features).
	   4. If “log2” is taken then max_features= log2(n_features).
	   5. If None, then max_features=n_features. By default, it takes “None” value.
	- max_depth: The max_depth parameter denotes the maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves contain just 
	  one data point (leading to overfitting) or until all leaves contain less than "min_samples_split" samples. By default, it takes “None” value.
	- min_samples_split: This tells about the minimum no. of samples required to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no.
	  If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split. By default, it takes the value "2".
	- min_samples_leaf: The minimum number of samples required to be at a leaf node. If an integer value is taken then consider min_samples_leaf as the minimum no. If float, then it 
	  shows the percentage. By default, it takes the value "1".
- The process of splitting only when there is a sufficient number of data points in the node is called Truncation.
- min_samples_split-The min_samples_split specifies the minimum number of data points a node should have for it to be considered for splitting.
- min_samples_leaf takes care of the minimum number of samples required to be at a leaf node.
- min_sample_split tells above the minimum no. of samples reqd. to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then 
  it shows percentage. By default, it takes the value “2”.
- min_sample_leaf is the minimum number of samples required to be at a leaf node. If an integer value is taken then consider - -min_samples_leaf as the minimum no. If float, then it shows 
  percentage. By default, it takes the value “1”.
-  the problems with manual hyperparameter tuning are as follows:
	Split into train and test sets: Tuning a hyperparameter makes the model 'see' the test data. Also, the results are dependent upon the specific train-test split.
	Split into train, validation and test sets: The validation data would eat into the training set.
- In the cross-validation technique, you split the data into train and test sets and train multiple models by sampling the train set. Finally, you can use the test set to test the 
  hyperparameter once.
- Specifically, you can apply the k-fold cross-validation technique, where you can divide the training data into k-folds/groups of samples. If k = 5, you can use k-1 folds to build the
  model and test it on the kth fold. 
- It is important to remember that k-fold cross-validation is only applied on the train data. The test data is used for the final evaluation. One extra step that we perform in order to 
  execute cross-validation is that we divide the train data itself into train and test (or validation) data and keep changing it across "k" no. of folds so that the model is more 
  generalised. 
- You played around with different values of different hyperparameters using GridSearchCV(). This function helped you try out different combinations of hyperparameters which ultimately 
  eased your process of figuring out these best values. It is, however, important to note that the values tried out in the demonstration above may have not necessarily given the best 
  results in terms of accuracy. 
- Decision Tree Regression:
- For continuous output variables? You calculate the weighted mean square error (WMSE) of data sets (before and after splitting) in a similar manner as you do for linear regression models. 
- low MSE = low Variance = High homoginity (preferable)
- In decision tree regression, each leaf represents the average of all the values as the prediction as opposed to taking an majority vote in classification trees. 
- MSE = sum((y(i)-ymean)^2)/N	The MSE is used to measure the homogeneity in regression where the target variable is continuous.
- A higher value of MSE means that the data values are dispersed widely around mean, and a lower value of MSE means that the data values are dispersed closely around mean and this is 
  usually the preferred case while building a regression tree.
- The regression tree building process can be summarised as follows:
	Calculate the MSE of the target variable.
	Split the data set based on different rules obtained from the attributes and calculate the MSE for each of these nodes.
	The resulting MSE is subtracted from the MSE before the split. This result is called the MSE reduction.
	The attribute with the largest MSE reduction is chosen for the decision node.
	The dataset is divided based on the values of the selected attribute. This process is run recursively on the non-leaf branches, until you get significantly low MSE and the node 
	becomes as homogeneous as possible.
	Finally, when no further splitting is required, assign this as the leaf node and calculate the average as the final prediction when the number of instances is more than one at a 
	leaf node
- So, you need to split the data such that the weighted MSE of the partitions obtained after splitting is lower than that obtained with the original or parent data set. In other words, 
  the fit of the model should be as ‘good’ as possible after splitting. As you can see, the process is surprisingly similar to what you did for classification using trees.
- One of the major drawbacks of decision trees, that is, overfitting, and the various methods that can be used to avoid it. 
- You learnt that decision trees are prone to overfitting. There are two ways to avoid overfitting: truncation and pruning.
- In truncation, you let the tree grow only to a certain size, while in pruning, you let the tree grow to its logical end and then you chop off the branches that do not increase the 
  accuracy on the validation set.
- There are various hyperparameters in the DecisionTreeClassifier that let you truncate the tree, such as minsplit, max_depth, etc.
- You also learnt about the effect of various hyperparameters on the decision tree construction.
 
- ENSEMBLES:
- In random forests, one of the most popular algorithms in machine learning. Random forests use a technique known as bagging, which is an ensemble method. So, before you learn about 
  random forests, let's first understand ensembles.
- An ensemble refers to a group of things viewed as a whole rather than individually. In an ensemble, a collection of models is used to make predictions, rather than individual models. 
  Arguably, the most popular in the family of ensemble models is the random forest, which is an ensemble made by the combination of a large number of decision trees.
- In principle, ensembles can be made by combining all types of models. An ensemble can have a logistic regression, a neural network, and a few decision trees working in unison.
- Now, before you understand how ensembles work, the following questions may arise:
	How does a collection of models work better than individual models?
	How do you choose individual models to form an ensemble such that it is better than any of the individual models?
- Diversity and Acceptability:  (independency and probability of correct prediction shd be >0.5)
- Ensembles of models are somewhat analogous to teams of individual players. If you were to choose a football team, you would take the following two steps:
- Choose people with different skill sets, such as defenders, attackers and a goalkeeper, to ensure diversity
- Choose good players, i.e., ensure that all players are acceptable from the point of view of their skill sets (and at least better than a regular person)
- Diversity ensures that the models serve complementary purposes, which means that the individual models make predictions independent of each other.
- For example, a random forest is an ensemble with a large number of trees as individual models. Diversity ensures that even if some trees overfit, the other trees in the ensemble will 
  neutralise the effect. The independence among the trees results in a lower variance of the ensemble compared to a single tree. Ensembles are more robust to the choice of the training 
  data which makes them more stable and less prone to high variance and overfitting. We will soon discuss how the learning algorithm is designed to achieve independence and how it is 
  beneficial.
- Acceptability implies that each model is at least better than a random model. This is a pretty lenient criterion for each model to be accepted into the ensemble, i.e., it has to be at 
  least better than a random guesser.
- There are a number of ways in which you can bring diversity among your models you plan to include in your ensemble.
	Use different subsets of training data
	Use different training hyperparameters
	Use different types of classifiers
	Use different features
- More the diversity, more diverse will be the results in the ensemble and hence, the ensemble will be equipped to handle and make predictions for a more diverse range of attributes. 
  If the diversity is reduced, the models will give similar results and hence the ensemble performance might not be much better than the individual models.
- If each individual model is acceptable, i.e., if it is wrong with a probability of less than 50%, you can show that the probability of the ensemble being wrong (i.e.,
  the majority vote going wrong) will be much less than that of any individual model. In this way your chance of getting the prediction correct will be higher as compared to the 
  individual models in the ensemble as they pool the opinion of each weak learner 
- Also, the ensembles cannot be misled by the assumptions made by individual models. For example, ensembles (particularly random forests) successfully reduce the problem of overfitting. 
  If a decision tree in an ensemble overfits, you let it. Chances are extremely low that more than 50% of the models are overfitted. Ensembles ensure that you do not put all your eggs in 
  one basket.
- If you can prove that the probability of more than half of the models making a wrong prediction is less than that of any of the individual models, you will know that the ensemble is a 
  better choice than any of the individual models. 
- It is important to remember that each model in an ensemble is acceptable, i.e., the probability of each model being wrong is less than 0.5
  (as a random binary classification model is correct 50% of the time).
- The probability of more than half of the models in an ensemble making the wrong prediction is significantly less than 0.5, i.e., less than a random model.
- Notice how the ensemble has a higher probability of being correct and a lower probability of being incorrect than any of the individual models (0.78 > 0.70 and 0.216 < 0.30). 
  In this way, you can also calculate the probabilities of the ensemble being correct and incorrect with 4, 5, 100, 1000, and even a million individual models.
  The difference in probabilities will increase with an increasing number of models, thus improving the overall performance of the ensemble.

- Ensembling:
- let’s look at some of the popular approaches to ensembling, such as voting, stacking, blending, boosting and bagging.
- Voting combines the output of different algorithms by taking a vote. In the case of a classification model, if the majority of the classifiers predict a particular class, then the 
  output of the model would be the same class. In the case of a regression problem, the model output is the average of all the predictions made by the individual models. In this way, 
  every classifier/regressor has an equal say in the final prediction.
- Another approach to carry out manual ensembling is to pass the outputs of the individual models to a level-2 classifier/regressor as derived meta features, which will decide what 
  weights should be given to each of the model outputs in the final prediction. In this way, the outputs of the individual models are combined with different weightages in the final 
  prediction. This is the high-level approach behind stacking and blending.
- Boosting is one of the most popular approaches to ensembling. It can be used with any technique and combines the weak learners into strong learners by creating sequential models such 
  that the final model has higher accuracy than the individual models. You saw the example shown below to see intuitively how adaptive boosting works.

- Bagging (Bootstrapped Aggregation) :
- Bagging creates different training subsets from the sample training data with replacement, and an algorithm with the same set of hyperparameters is built on these different subsets of 
  data. In this way, the same algorithm with a similar set of hyperparameters is exposed to different parts of data, resulting in a slight difference between the individual models. 
  The predictions of these individual models are combined by taking the average of all the values for regression or a majority vote for a classification problem.
- Bagging works well with high variance algorithms and is easy to parallelise. By high variance, we mean algorithms which change a lot with slight changes in the data as a result of which
  these algorithms very easily overfit if not controlled. If you recall, decision trees are very prone to overfitting if we don't tune the hyperparameters well. Hence, bagging works very
  well for high-variance models like decision trees.
- However,  it has got some disadvantages as well. In this approach, you cannot really see the individual trees one by one and figure out what is going on behind the ensemble as it is a 
  combination of n number of trees working together. This leads to a loss of interpretability. Also, it does not work well when any of the features dominate because of which all the trees 
  look similar and hence the property of diversity in ensembles is lost. Sometimes bagging can be computationally expensive and is applied depending on the case.
- So far you have seen handling classification problems with ensembles. But remember that ensembles work well for regression problems as well. The working is almost similar except for 
  the aggregation of models the average of the predictions is taken for regression instead of majority voting for classification ensembles.

- Random Forests:
- Bagging is one such ensemble model which creates different training subsets from the training data with replacement
- Bagging chooses random samples of observations from a data set. Each of these samples is then used to train each tree in the forest. However, keep in mind that bagging is only a 
  sampling technique and is not specific to random forests.
- In the bagging type of ensembles, random forests are by far the most successful. They are essentially ensembles of a number of decision trees. You can create a large number of models 
  (say, 100 decision trees), each one on a different bootstrap sample from the training set. To get the result, you can aggregate the decisions taken by all the trees in the ensemble.
- Bootstrapping refers to creating bootstrap samples from a given data set. A bootstrap sample is created by sampling the given data set uniformly and with replacement.
  A bootstrap sample typically contains about 40–70% data from the data set. Aggregation implies combining the results of different models present in the ensemble.
- You learnt that a random forest selects a random sample of data points (bootstrap sample) to build each tree and a random sample of features while splitting a node. Randomly selecting 
  features ensures that each tree is diverse and that some prominent features are dominating in all the trees making them somewhat less similar.
- Suppose you want to build a random forest of 10 decision trees. First, you will create 10 bootstrap samples from the data, and then, you will train each tree on a different bootstrap 
  sample. Recall that in a decision tree every data point passes from the root node to the bottom until it is classified in a leaf node. A similar process takes place in random forests as 
  well while making predictions. Each data point passes through different trees in the ensemble which are built on different training and feature subsets. The final outcome of these trees 
  are then combined either by taking the most frequent class prediction in case of a classification problem or average in case of a regression problem.
- A random subset of observations is chosen every time a new tree is built in a forest: A different random subset of observations is chosen, which is called the bootstrap sample, 
											for each tree that is to be built in the forest. This is called bootstrapping.
- A random subset of features is chosen every time a node is being split inside a tree: After the bootstrap sample is selected, tree building starts, and a random subset of features is 
											selected at each node in order to split it. This is what makes random forests better than a simple 
											bagging algorithm.
- Apart from the general advantages of ensembles, random forests (or black box models) have significant advantages owing to their origins in decision trees and other linear models. 
- It is worth reiterating that random forests have been much more successful than decision trees. In fact, as you learnt that ensembles are better than individual models 
  (assuming diversity and acceptability), you can say that random forests are almost always better than decision trees, only if the trees are diverse and acceptable.

- The advantages of random forests over decision trees and other linear models are manifold;
	Diversity: Diversity arises because each tree is created with a subset of the attributes/features/variables, i.e., not all the attributes are considered while making each tree; 
		   the choice of the attributes is random. This ensures that the trees are independent of each other.
	Stability: Stability arises because the answers given by a large number of trees average out. A random forest has a lower model variance than an ordinary individual tree.
	Immunity to the curse of dimensionality: Since each tree does not consider all the features, the feature space (the number of features that a model has to consider) reduces. 
						 This makes an algorithm immune to the curse of dimensionality. Also, a large feature space causes computational and complexity issues.
	Parallelization: You need a number of trees to make a forest. Since two trees are independently built on different data and attributes, they can be built separately. 
			 This implies that you can make full use of your multi-core CPU to build random forests. Suppose there are 4 cores and 100 trees to be built; each core can build 
			 25 trees to make a forest.
	Testing/training data and the OOB (out-of-bag) error: You should always avoid violating the fundamental tenet of learning: 'Not testing a model on what it has been trained on’. 
  	 While building individual trees, you can choose a random subset of the observations to train them. If you have 10,000 observations, each tree may only be built from 7,000 (70%)
	 randomly chosen observations. OOB is the mean prediction error on each training sample xᵢ, using only the trees that do not have xᵢ in their bootstrap sample used for building 
	 the model. This is very similar to a cross-validation (CV) error. In a CV error, you can measure the performance on the subset of data that the model has not seen before.

- In fact, it has been proven that using an OOB estimate is as accurate as using a test data set of a size equal to the training set.
- Thus, the OOB error omits the need for set-aside test data (though you can still work with test data like you have been doing, at the cost of eating into the training data).
- Bootstrapping implies that each tree in a random forest is built on randomly chosen observations: The word ‘random’ in random forests pertains to the random choice of bootstrapped observations.
- While considering a split at a node, a random set of attributes is considered: Random choice of attributes at each split of a tree ensures that the prominent features do not appear in 
										 every tree, thus ensuring diversity.
- Random choice of attributes while splitting at nodes ensures diversity in a random forest: Random choice of attributes ensures that the prominent features do not appear in every tree, 
											     thus ensuring diversity.
- The core idea behind bagging is considering a majority score rather than committing to a set of assumptions made by a single model: Trees are typically unstable: If you have only one 
	tree, you have to rely on the decision that it makes. The decision made by a single tree (on unseen data) majorly depends upon the training data, as trees are unstable. 
	On the other hand, in a forest, even if a few trees are unstable, averaging out their decisions ensures that you do not make mistakes because of the unstable behaviour of a 
	few trees.
- In terms of accuracy, is a random forest always better than a decision tree?  'NO'
  While it is well known that random forests are better than a single decision tree in terms of accuracy, it cannot be said that they are better than every possible decision tree;
  the only issue is that it is more difficult to build a decision tree that is better than a random forest. In fact, there may be several trees that provide better predictions on unseen 
  data.
- Gini score(information gain or purity gain) = 1- Gini impurity.
- The OOB (out-of-bag) error is almost as good as the cross-validation error. The final prediction is the aggregation of all the predictions of individual decision trees. 
  Remember that each tree in a random forest is trained on a random subset of the training set, which is called a bootstrapped sample. This means that for each sample (observation), 
  there are several trees that did not include that sample, and for these trees, this sample is unseen. Let’s understand this better.
- Suppose there are N = 100 observations with M = 15 features, and the outcome variable is a categorical variable Y. Also, you build a random forest with 50 trees. 
  The OOB is calculated as follows: For each observation Ni, Ni is passed to all the trees that did not have it in their training. These trees then predict the class of Ni. 
  The final prediction for Ni is decided by a majority vote.
- Now let’s apply this to N1. Suppose 10 trees did not have N1 in their training. So these 10 trees make their prediction for N1. Let’s say four trees predicted 0, and the 
  other six predicted 1 as the output. The final prediction for N1 will be 1.
- Next, we move on to N2. Suppose 15 trees did not have N2 in their training. So these 15 trees make their prediction for N2. Let’s say 12 predicted 0, and the remaining three
   trees predicted 1 as the ouput. The final prediction for N2 will be 0.
- This is done for each observation in the training set. Once all the predictions for each observation are calculated, the OOB error is calculated as the number of observations predicted 
  wrongly as a proportion of the total number of observations. 
- Only the training set is used while calculating the OOB error, which is why it gives a good idea of model performance on the unseen data without using a test set.
- All the observations of the training set are used to calculate the OOB error.
- Random forests use multiple trees, reduce variance and allow for more exploration of feature combinations.
- The importance of features in random forests, sometimes called ‘Gini importance’ or ‘mean decrease impurity’, is defined as the total decrease in node impurity 
  (it is weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all the trees of the ensemble.
- For each variable, the sum of the Gini decreases across every tree of the forest and is accumulated every time that variable is chosen to split a node. The sum is divided by the number 
  of trees in the forest to give an average.
- Changed in version 0.22: cv default value if None changed from 3-fold to 5-fold.
- n_estimatorsint, default=100
	The number of trees in the forest.
	Changed in version 0.22: The default value of n_estimators changed from 10 to 100 in 0.22.
- To summarise, you learnt how to build a random forest in sklearn. Apart from the hyperparameters that you have in a decision tree, there are two more hyperparameters in random forests: 
  max_features and n_estimators. The effects of both the hyperparameters are briefly summarised below.
- The effect of max_features: You learnt that there is an optimal value of max_features, i.e, at very low values, the component trees are too simple to learn about anything useful,
   while at extremely high values, the component trees become similar to each other (and violate the 'diversity' criterion).
- The effect of n_estimators: When you observe the plot of n_estimators and training and test accuracies, you will see that as you increase the value of n_estimators, the accuracies of 
  both the training and test sets gradually increase. More importantly, the model does not overfit even when its complexity is increasing. This is an important benefit of random forests: 
  You can increase the number of trees as much as you like without worrying about overfitting (only if your computational resources allow). 
- At very low values of max_features (e.g. 2), both the training and test accuracies will be low; both accuracies will gradually increase with max_features up to a certain point, 
  while at extremely high values (e.g. 19), both will go down again.
- However, you know that decision trees have their own limitations, and you need to overcome them to use random forests in order to exploit the predictive power of decision 
  trees and obtain better results.
- Decision trees are high variance models and that they change quite rapidly with small changes in the data.
- Random forests definitely gave a great leverage in the results as compared to both logistic regression and decision trees with much less effort. 
  It has exploited the predictive power of decision trees and learnt much more than a single decision tree could do alone.
- Random forests use multiple trees, reduce variance, allow for more exploration of feature combinations.
- Training process of each tree in a Random forest is same as a decision tree except with the difference that at each node in the tree only a random selection of 
  features is used for the split in that node.
- Each data point is considered to calculate OOB error, but for each of the points, only the trees which didn't have that data point in their bootstrap sample 
  will be considered to calculate the OOB error.
- we take all the trees that attribute was present in and aggregate the homogeneity measures to calculate feature importance. We basically select all the trees 
  the attribute was present in and calculate the ΔHomogeneity on the attribute split. We then take an average of all of these ΔHomogeneity to arrive at the final feature importance.
-
- BOOSTING:
- Most popular boosting algorithms – AdaBoost, Gradient Boosting and XGBoost.
-‘The strength of unity lies in the diversity’ This saying holds the same meaning in the world of machine learning. Ensemble models bring us that flavour of diversity to create powerful
  models that can handle complex problems. It is a combination of models, each of which are trained to solve the same problem and provide the best result at the end.
- For a machine learning task (classification or regression), you need a model that identifies the necessary patterns in the data and does not overfit. In other words, ‘the models should 
  not be so simple as to not be able to identify even the important patterns present in the data; on the other hand, they should not be so complex as to even learn the noise present in 
  the data set’.
- This solution can be arrived at either through a single model or an ensemble, i.e., a collection of models. By combining several models, ensemble learning methods create a strong learner,
  thus reducing the bias and/or variance of the individual models.
- Bagging is one such ensemble model which creates different training subsets from the training data with replacement. Then, an algorithm with the same set of hyperparameters is built on 
  these different subsets of data.
- In this way, the same algorithm with a similar set of hyperparameters is exposed to different subsets of the training data, resulting in a slight difference between the individual models.
  The predictions of these individual models are combined by taking the average of all the values for regression or a majority vote for a classification problem. Random forest is an 
  example of the bagging method. 
- Bagging works well when the algorithm used to build our model has high variance. This means the model built changes a lot even with slight changes in the data. As a result, these 
  algorithms overfit easily if not controlled. Recall that decision trees are prone to overfitting if the hyperparameters are not tuned well. Bagging works very well for high-variance
  models like decision trees.
- Boosting is another popular approach to ensembling. This technique combines individual models into a strong learner by creating sequential models such that the final model has a 
 higher accuracy than the individual models. 
- These individual models are connected in such a way that the subsequent models are dependent on errors of the previous model and each subsequent model tries to correct the errors of
  the previous models.

- In bagging the base models are parallelly connected to reduce the overall variance of the resulting strong model.
- In boosting the base models are sequentially connected to reduce the overall bias of the resulting strong model.

-Weak Learners:
- As discussed earlier, boosting is an approach where the individual models are connected in such a way that they correct the mistakes made by the previous models. Here, these individual
  models are called weak learners. 
- Till now, all the models you have learnt are strong learners, where each model performs well on any task that it is assigned to do – classification or regression. 
- Weak learner, on the other hand, refers to a simple model which performs at least better than a random guesser (the error rate should be lesser than 0.5). It primarily identifies only 
  the prominent pattern(s) present in the data and thus is not capable of overfitting. In boosting, such weak learners can be used to build your ensemble.
- In boosting, any model can be a weak learner – linear regression, decision tree or any other model, but more often than not, tree methods are used here.
  Note: Decision stump is one such weak learner when talking about a shallow decision tree having a depth of only 1.
- To summarize: Weak learners are combined sequentially such that each subsequent model corrects the mistakes of the previous model, resulting in a strong overall model that gives good
  predictions.
- Through weak learners, you can do the following:
	Reduce the variance of the final model, making it more robust (generalisable) 
	Train the ensemble quickly resulting in faster computation time
- The weak learners in boosting algorithm have an error rate less than 0.5 and each model corrects the mistakes of the previous trees.

- ADAPTIVE BOOSTING:
- In random forest each time you make a tree ,you make a full sized tree. Some tree might be bigger than others , but there is no pre determined maximum depth.
- The three ideas behind adaboost are:
	1. Adaboost combines a lot of weak learners to make classifications. The weak learners are almost always a stump.
	2. Some stumps will get more say in the classifications than others.
	3. Each stump is made by taking the previous stumps mistakes into account.
- Finally Adaboost makes classifications by adding the amount of say of all the stumps which classifies the patients as heart disease and not having heart disease seperately and the final 
  prediction will be given the class whose sum is maximum.
-  Before starting with a numerical example to understand AdaBoost, let’s see an overview of the steps that need to be taken in this boosting algorithm:
	AdaBoost starts with a uniform distribution of weights over training examples, i.e., it gives equal weights to all its observations. These weights tell the importance of each 
	datapoint being considered.
	We start with a single weak learner to make the initial predictions.
	Once the initial predictions are made, patterns which were not captured by the previous weak learner are taken care of by the next weak learner by giving more weightage to the 
	misclassified datapoints.
	Apart from giving weightage to each observation, the model also gives weightage to each weak learner. More the error in the weak learner, lesser is the weightage given to it. 
	This helps when the ensembled model makes final predictions.
	After getting the two weights for the observations and the individual weak learners, the next weak learner in the sequence trains on the resampled data (data sampled according to 
	the weights) to make the next prediction.
	The model will iteratively continue the steps mentioned above for a pre-specified number of weak learners. 
	In the end, you need to take a weighted sum of the predictions from all these weak learners to get an overall strong learner. 
- Gini score(information gain or purity gain) = 1- Gini impurity.
- To summarise, here are the major takeaways from this video:
- In AdaBoost, we start with a base model with equal weights given to every observation. In the next step, the observations which are incorrectly classified will be given a higher weight 
  so that when a new weak learner is trained, it will give more attention to these misclassified observations.
- In the end, you get a series of models that have a different say according to the predictions each weak model has made. If the model performs poorly and makes many incorrect predictions,
  it is given less importance, whereas if the model performs well and makes correct predictions most of the time, it is given more importance in the overall model.
- The say/importance each weak learner — in our case the decision tree stump — has in the final classification depends on the total error it made. 
  α = 0.5 * ln( (1 − Total error)/Total error ) 
- The value of the error rate lies between 0 and 1. So, let’s see how alpha and error is related.
- When the base model performs with less error overall, then, as you can see in the plot above, the α is a large positive value, which means that the weak learner will have a high say 
  in the final model. 
- If the error is 0.5, it means that it is not sure of the decision, then the α = 0, i.e., the weak learner will have no say or significance in the final model.
- If the model produces large errors (i.e., close to 1), then α is a large negative value, meaning that the predictions it makes are incorrect most of the time. Hence, this weak 
  learner will have a very low say in the final model. 
- After calculating the say/importance of each weak learner, you must determine the new weights of each observation present in the training data set. Use the following formula to 
  compute the new weight for each observation:
	new sample weight for the incorrectly classified observation = original sample weight * e^α
	new sample weight for the correctly classified observation = original  sample weight * e^(−α)
- After calculating, we normalise these values to proceed further using the following formula:
- Normalized weights =p(xi)/∑p(xi) , where p(xi) is the weight of each observation.
- The samples which the previous stump incorrectly classified will be given higher weights and the ones which the previous stump classified correctly will be given lower weights. 
- If the model performs poorly and makes many incorrect predictions, it is given less significance, whereas if the model performs well and makes correct predictions most of the time, 
  it is given more significance in the overall model.   
- 1. Whenever we start with a new model all the samples of the dataset need to have equal distribution (1/n) and of the same size.
- 2. Also, the new learner should focus more on the samples which are incorrectly classified at the previous iteration.
- To handle both these bottlenecks, a new dataset will be created by randomly sampling the weighted observations
- We create a new and empty dataset that is the same size as the original one. Then we take the distribution of all the updated weights created by our first model.
- To fill our new empty dataset, we select numbers between 0 and 1 at random. The position where the random number falls determine which observation we place in our new dataset.
- Due to the weights given to each observation, the new data set will have a tendency to contain multiple copies of the observation(s) that were misclassified by the previous tree and 
  may not contain all observations which were correctly classified. 
- After doing this, the initial weights for each observation will be 1/n, thus we can continue the same process as learnt earlier to build the next weak learner.
- This will help the next weak learner give more importance to the incorrectly classified sample so that it can correct the mistake and correctly classify it now. This process will be 
  repeated till a pre-specified number of trees are built, i.e., the ensemble is built. 
- The AdaBoost model makes predictions by having each tree in the ensemble classify the sample. Then, the trees are split into groups according to their decisions. For each group, 
  the significance of every tree inside the group is added up. The final prediction made by the ensemble as a whole is determined by the sign of the weighted sum.
- NOTE: Weight is mentioned in two different contexts - one for each observation & the other is in context of the importance/say given to each weak learner in the model.
- to force classifiers (weak learners) to concentrate on observations that are difficult to classify correctly. 
- Practical advice: Before you apply the AdaBoost algorithm, you should remove the Outliers. Since AdaBoost tends to boost up the probabilities of misclassified points and there 
  is a high chance that outliers will be misclassified, it will keep increasing the probability associated with the outliers and make the progress difficult. Some of the ways to 
  identify outliers are:
	Boxplots
	Cook's distance
	Z-score.
- intuition behind boosting and AdaBoost algorithm in detail.
	AdaBoost starts with a uniform distribution of weights over training examples.
	These weights give the importance of the datapoint being considered.
	You will first start with a weak learner h1(x) to create the initial prediction.
	Patterns which are not captured by previous models become the goal for the next model by giving more weightage.
	The next model (weak learner) trains on this resampled data to create the next prediction.
	This process will be repeated till a pre-specified number of trees/models are built.
	In the end, we take a weighted sum of all the weak classifiers to make a strong classifier.

- Gradiant Boosting:
- Gradiant boost starts by making a single leaf insted of a tree or stump.
- This leaf represents an initial guess for the weights of all of the samples.
- While trying to predict a continuous value likee weight, the firat guess is the average value.
- like adaboost, this tree is based on the errors made by the previous tree.
- But unlike adaboost this tytree is larger than a stump.
- That said gradiant boost still restricts the size of the tree.
- Thus like adaboost, gradiant boost builds fixed sized trees based on the previous trees errors, but unlike adaboost, each tree can be a larger than a stump.
- Also like adaboost, gradiant boost scales the trees. However gradiant boost scales all the trees by the same amount.
- Then gradiant boost builds another tree based on the errors made by the previous tree.....and then it sacles the tree.....and gradiant boost continuous to build trees in this fashion
  until it has made the number of trees u had asked for, or additional trees failed to improve the fit.

- Steps in Gradiant boost regressor.
 - We start with the leaf that is the average value of the variable we want to predict.(in this case we want to predict the weight)
 - Then we add a tree based on the Residuals (the deference btw the observed value and the predicted values)
 - We scale the trees contributions to the final predictions with the Learning Rate.
 - Then we add another tree based on new Residuals
 - And we keep adding trees based on the errors made by the previous tree.

- GRADIENT BOOSTING:
- Gradient Boosting, like AdaBoost, trains many models in a gradual, additive and sequential manner. However, the major difference between the two is how they identify and handle the 
  shortcomings of weak learners. In AdaBoost, more weight is given to the datapoints which are misclassified/wrongly predicted earlier. Gradient Boosting performs the same by using 
  gradients in the loss function
- NOTE: New prediction = Initial prediction + Learning Rate*Residuals

- XGBoosting:
- Which of the following is/are a mandatory data pre-processing step(s) for an XGBoost model?
  Remove highly correlated predictors-Decision trees are by nature immune to multicollinearity. Since boosted trees use individual decision trees, they also are unaffected by 
					multicollinearity.
  One-hot encoding or dummy variable creation of categorical predictors- Yes, converting categorical data to numeric format is required.
  Normalization and scaling- For the decision tree algorithm, normalization is not necessary in the model training procedure.
- In XGBoost we have different parameters which help us in regularising the overall growth of our decision tree.
	As we increase the values of lambda(λ), the chances of pruning of the branches increases: Increasing the values of lambda(λ) decreases our similarity score. Lower values 
	of the similarity score will decrease the value of the gain which will lead to pruning
	As we increase the values of gamma(γ), the chances of pruning of the branches increases: A branch containing the terminal node is pruned when the gain < γ (or gain-γ = negative), 
	therefore if we increase the values of gamma(γ), the chances of pruning increases
- Hyperparameters - Learning Rate, Number of Trees and Subsamplingλt, the learning rate, is also known as shrinkage. It can be used to regularize the gradient tree boosting algorithm.
  λt typically varies from 0 to 1. Smaller values of λtlead to a larger number of trees T (called n_estimators in the Python package XGBoost). This is because, with a slower 
  learning rate, you need a larger number of trees to reach the minima.  
  This, in turn, leads to longer training time. On the other hand, if λt is large, we may reach the same point with a lesser number of trees (n_estimators), but there is the risk of 
  actually missing the minima altogether (i.e., cross over it) because of the long stride taken at each iteration.
- Some other ways of regularisation are explicitly specifying the number of trees T and doing subsampling. Note that you should not tune both λt and number of trees T together 
  since a high λt implies a low value of T and vice-versa.
- Subsampling is training the model in each iteration on a fraction of data (similar to how random forests build each tree).  A typical value of subsampling is 0.5 while it ranges 
  from 0 to 1. In random forests, subsampling is critical to ensure diversity among the trees, since otherwise, all the trees will start with the same training data and therefore 
  look similar. This is not a big problem in boosting since each tree is any way built on the residual and gets a significantly different objective function than the previous one.
- γ,Gamma is a parameter used to control the pruning of the tree. A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the 
  minimum loss reduction required to make a split and makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
- Apart from the previously mentioned hyperparameters, there are other parameters of decision trees like the depth of the tree, the minimum number of samples required for split, etc.







******************************************************************************************************************************************************************************************
Interview Qstns:
1. Correlation and Covariance:
  - Covariance = (∑(X-Xmean)(Y-Ymean))/(n-1)
  - Covariance: -quantifies the trends of relation btw x and y.
		-The main idea of covariance is that it can classifty three types of relationship:- positive trend(x is directly proportionall to y), negative trend and no trend.
		- it is sensitive to scalling. Hence the magnitude of covariance value is of no use unless for calculating correlation.
		- Value ranges from -infy to +infy.
		- The covariance value doesnt tell us if the slop of the line representing the relationship is steep or not steep.
		- Most importantly the covariance value doesnt tell us if the points are relatively close to the line or relatively far from the line.
		- The covariance of the Gene x with itself is the same thing as the estimated variance for Gene x.
		- The sensitivity of the scale also prevents the covariance value from telling us if the data are close to the line that represnets the relationship or far from it.	

  - Correlation: quantifies the strength of relation btw x and y.
		 not sensitive to scalling.
		- Value ranges from -1 to +1.
		- The correlation value tell us if the points are relatively close to the line or relatively far from the line.(how eaxctly data points hug to the line).

  - correlation = (covariance(Gene x, Gene y))/(sqrt(variance(gene x)*sqrt(variance(gene y)))
  - The more data we have the smaller p-value and the more confidence we have on our inferences.
  -

******************************************************************************************************************************************************************************************
- Decision Trees:
- In general a decision tree asks a qstn and then classifies based on yes or no.
- Steps:
- Calculate all the gini impurity scores
- If the node itself has the lowest score, then there is no point in separating the patients anymore and it becomes a leaf node
- If separation of data results in an improvement, then pick the separation with the lowest impurity value.

- Hyperparameters in Decision Trees:
	1. criterion{“gini”, “entropy”}, default=”gini”
	2. max_depth int, default=None
	3. min_samples_split int or float, default=2
	4. min_samples_leaf int or float, default=1
	5. max_features int, float or {“auto”, “sqrt”, “log2”}, default=None
	6. max_leaf_nodes int, default=None
	7. min_impurity_decrease float, default=0.0

******************************************************************************************************************************************************************************************
- Random Forest:
- Bootstrapping the data plus using the aggregate to make a decision is called Bagging.
- Create a bootstrapped dataset
- Create a decision tree using the bootstrapped dataset, but only use a random set of variables(or columns) at each step.
- Now go back to step 1 and repeat : Make a new bootstrapped dataset and build a tree considering a subset of variables at each step.
- Using a bootstrapped sample and considering only a subset of variables at each step results in a wide variety of trees. The variety is what makes a random forest more effective than
  individual decision trees

- Hyperparameters in Random Forest:
	1. n_estimators int, default=100 (The number of trees in the forest.)
	2. bootstrap bool, default=True (Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.)
	3. oob_score bool, default=False (Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.)
	4. max_samples int or float, default=None (If bootstrap is True, the number of samples to draw from X to train each base estimator.)
	1. criterion{“gini”, “entropy”}, default=”gini”
	2. max_depth int, default=None
	3. min_samples_split int or float, default=2
	4. min_samples_leaf int or float, default=1
	5. max_features int, float or {“auto”, “sqrt”, “log2”}, default=None
	6. max_leaf_nodes int, default=None
	7. min_impurity_decrease float, default=0.0	

******************************************************************************************************************************************************************************************
- Adaboost:
- The three ideas behind adaboost are:
	1. Adaboost combines a lot of weak-learners to make classification. The weak learners are almost always stumps.
	2. Some stumps get more say in the classification than others
	3. Each stump is made by taking the previous stumps mistakes into account
- Finally for a test data point we segregate the stumps which says yes and No separately and add up their amount of say. The group with heighest amount of say is the final class predicted.
- Here we give equal weitage to all samples at first and then the misclassified sample will get heigher weight and correctly classified sample weight will decrease (e^alpha and e^-alpha)
- Hence in the next sampling the misclassified sample will appear more times.

- Hyperparameters in Adaboost:
	1. n_estimators int, default=50
	2. learning_rate float, default=1.0 (Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier.)

******************************************************************************************************************************************************************************************
- GradientBoost:
- It starts with a leaf (it takes avg val of target variable or mode)
- Calculates residuals at every step (residual= actual-predicted) 
- build a decision tree using residuals and scale it using learning rate.
- Now again calculate the new residuals and build another decision tress using the new residuals and further scale it with the learning rate.
- At every step we can see that the residuals go on decreasing indicating that we are moving towards the right direction

- Hyperparameters in Gradient Boost:
	1. n_estimators int, default=100
	2. learning_rate float, default=0.1
	2. max_depth int, default=None
	3. min_samples_split int or float, default=2
	4. min_samples_leaf int or float, default=1
	5. max_features int, float or {“auto”, “sqrt”, “log2”}, default=None
	6. max_leaf_nodes int, default=None
	7. min_impurity_decrease float, default=0.0

******************************************************************************************************************************************************************************************
- XGBoost:
- While building XGBoost trees for regression we calculate Similarity scores and Gain to determine how to split the data
- and we prune the tree by calculating the difference between Gain values and a user defined Tree Complexity parameter  gama
- If this difference is negative then we purne, if positive then do not prune. This way we walk our way up the tree.
- then we calculate the output values for the remaining leaves 
- Lastly lambda is a regularization Parameter and when lambda>0 , it results in more pruning, by shrinking the similarity scores and it results in output values for the leaves.

- Hyperparameters for XGBoost:
	1. gamma (default 0) (A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a 
		split.)
	2. eta (Analogous to learning rate)
	3. lambda [default=1] (L2 regularization term on weights (analogous to Ridge regression))
	4. max_leaf_nodes
	5. max_depth [default=6]
	








