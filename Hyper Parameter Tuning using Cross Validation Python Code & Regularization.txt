*********************************************************************************************************************
# K-Fold Cross Validation

lm=LinearRegression()
folds=KFold(n_splits=5,shuffle=True,random_state=100)
scores=cross_val_score(lm,X_train,y_train,scoring='r2',cv=folds)
scores

Output:array([0.59930574, 0.71307628, 0.61325733, 0.62739077, 0.6212937 ])

*********************************************************************************************************************

# Hyperparameter Tuning Using Grid Search Cross-Validation

folds=KFold(n_splits=5,shuffle=True,random_state=100)
hyper_params=[{'n_features_to_select':list(range(1,14))}]

lr=LinearRegression()
lr.fit(X_train,y_train)
rfe=RFE(lr)
model_cv=GridSearchCV(estimator=rfe,param_grid=hyper_params,scoring='r2',verbose=1,return_train_score=True,cv=folds)
model_cv.fit(X_train,y_train)

cv_results=pd.DataFrame(model_cv.cv_results_)
cv_results

*********************************************************************************************************************

# plotting cv results

plt.figure(figsize=(16,6))
plt.plot(cv_results["param_n_features_to_select"], cv_results["mean_test_score"])
plt.plot(cv_results["param_n_features_to_select"], cv_results["mean_train_score"])
plt.xlabel('number of features')
plt.ylabel('r-squared')
plt.title("Optimal Number of Features")
plt.legend(['test score', 'train score'], loc='upper left')

**********************************************************************************************************************
## REGULARIZATION

# Ridge Regression and Lasso Regression: (for Lasso just replace Ridge by Lasso, everything remains the same)

folds=KFold(n_splits=5,shuffle=True,random_state=100)
params={'alpha':[0.0001,0.001,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,20,50,100,500,1000]}

ridge=Ridge()

model_cv=GridSearchCV(estimator=ridge,param_grid=params,scoring='r2',cv=5,verbose=1,return_train_score=True)
model_cv.fit(X_train,y_train)

# To check the optimal alpha(ie.Lambda)

model_cv.best_params_
---output: {'alpha': 5}

# Now building the model using the optimal alpha:

ridge=Ridge(alpha=5)
ridge.fit(X_train,y_train)
ridge.coef_

# Predicting

y_pred_train=ridge.predict(X_train)
y_pred_test=ridge.predict(X_train)
y_pred_train

# Evaluation

ridge.score(X_train,y_train) #This will give us r2_score

# R2_score
r=r2_score(y_train,y_pred_train)

# MSE
MSE=np.sum(np.square(y_train,y_pred_train))/len(y_train)

# RMSE
RMSE=np.sqrt(np.sum(np.square(y_train,y_pred_train))/len(y_train))

****************************************************************************************************************************

